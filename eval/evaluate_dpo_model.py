"""
è¯„ä¼°DPOè®­ç»ƒåçš„æ¨¡å‹æ€§èƒ½
è®¡ç®—task success rateå’Œå…¶ä»–æŒ‡æ ‡
"""
import json
import sys
import re
import subprocess
import tempfile
import os
from pathlib import Path
from typing import Dict, List, Optional
import argparse

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from reward.compute import compute_task_score, compute_interrupt_cost, total_reward


def load_jsonl(path: Path) -> List[Dict]:
    rows = []
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            rows.append(json.loads(line))
    return rows


def extract_code_from_text(text: str) -> Optional[str]:
    """ä»æ–‡æœ¬ä¸­æå–Pythonä»£ç å—"""
    # å°è¯•æå–æ‰€æœ‰markdownä»£ç å—
    code_block_pattern = r'```(?:python)?\s*\n(.*?)```'
    matches = re.findall(code_block_pattern, text, re.DOTALL)
    
    if matches:
        # è¿‡æ»¤æ‰é”™è¯¯ä¿¡æ¯å’Œæµ‹è¯•ä»£ç 
        valid_blocks = []
        for code in matches:
            code = code.strip()
            # è·³è¿‡é”™è¯¯ä¿¡æ¯ï¼ˆæ›´ä¸¥æ ¼çš„æ£€æŸ¥ï¼‰
            if any(keyword in code for keyword in ["Traceback", "Error:", 'File "__test__', "Traceback (most recent call last)", "ZeroDivisionError", "ValueError", "Exception"]):
                continue
            # è·³è¿‡æµ‹è¯•ä»£ç ï¼ˆåŒ…å« unittest æˆ– test_ï¼‰
            if any(keyword in code for keyword in ["unittest", "test_", "TestCases", "class Test", "def test_"]):
                continue
            # ç¡®ä¿ä»£ç åŒ…å«å‡½æ•°å®šä¹‰
            if "def " not in code:
                continue
            valid_blocks.append(code)
        
        if valid_blocks:
            # å¦‚æœæœ‰å¤šä¸ªä»£ç å—ï¼Œé€‰æ‹©æœ€é•¿çš„ï¼ˆé€šå¸¸æ˜¯æœ€å®Œæ•´çš„ï¼‰
            # æˆ–è€…é€‰æ‹©åŒ…å«å‡½æ•°ä½“çš„é‚£ä¸ª
            best_code = None
            best_score = 0
            
            for code in valid_blocks:
                # è®¡ç®—åˆ†æ•°ï¼šå‡½æ•°ä½“è¡Œæ•°
                lines = code.split('\n')
                # æ£€æŸ¥æ˜¯å¦æœ‰å‡½æ•°å®šä¹‰
                has_def = any('def ' in line for line in lines)
                if not has_def:
                    continue
                
                # è®¡ç®—å‡½æ•°ä½“è¡Œæ•°ï¼ˆç¼©è¿›çš„è¡Œï¼‰
                body_lines = sum(1 for line in lines if line.strip() and line[0] in ' \t')
                score = len(code) + body_lines * 10  # é•¿åº¦ + å‡½æ•°ä½“è¡Œæ•°æƒé‡
                
                if score > best_score:
                    best_score = score
                    best_code = code
            
            if best_code:
                return best_code
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ€å¥½çš„ï¼Œè¿”å›æœ€é•¿çš„
            return max(valid_blocks, key=len)
    
    # å¦‚æœæ²¡æœ‰ä»£ç å—ï¼Œå°è¯•æå–å‡½æ•°å®šä¹‰ï¼ˆåŒ…æ‹¬å®Œæ•´å‡½æ•°ä½“ï¼‰
    # åŒ¹é…ä» def å¼€å§‹åˆ°ä¸‹ä¸€ä¸ª def æˆ–æ–‡ä»¶ç»“å°¾ï¼Œä½†éœ€è¦åŒ…å«å‡½æ•°ä½“
    def_positions = [m.start() for m in re.finditer(r'^def\s+\w+', text, re.MULTILINE)]
    if def_positions:
        # å–ç¬¬ä¸€ä¸ªå‡½æ•°
        start = def_positions[0]
        # æ‰¾åˆ°ä¸‹ä¸€ä¸ª def æˆ–æ–‡ä»¶ç»“å°¾
        next_def = def_positions[1] if len(def_positions) > 1 else len(text)
        # æå–å‡½æ•°ï¼ˆåŒ…æ‹¬å‡½æ•°ä½“ï¼‰
        func_code = text[start:next_def].strip()
        # ç¡®ä¿å‡½æ•°ä½“ä¸ä¸ºç©ºï¼ˆè‡³å°‘æœ‰ä¸€è¡Œç¼©è¿›çš„å†…å®¹ï¼‰
        lines = func_code.split('\n')
        if len(lines) > 1:
            # æ£€æŸ¥æ˜¯å¦æœ‰å‡½æ•°ä½“ï¼ˆæœ‰ç¼©è¿›çš„è¡Œï¼‰
            has_body = any(line.strip() and line[0] in ' \t' for line in lines[1:])
            if has_body:
                return func_code
    
    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œå°è¯•æå–æ‰€æœ‰è¿ç»­çš„ä»£ç è¡Œï¼ˆä»¥ import æˆ– def å¼€å¤´ï¼‰
    code_lines = []
    in_code_block = False
    for line in text.split('\n'):
        stripped = line.strip()
        if stripped.startswith(('import ', 'from ', 'def ', 'class ', '#')):
            in_code_block = True
            code_lines.append(line)
        elif in_code_block:
            if stripped == '' or line[0] in ' \t' or stripped.startswith('#'):
                code_lines.append(line)
            else:
                break
    
    if code_lines:
        return '\n'.join(code_lines).strip()
    
    return None


def score_code_passfail(code: str, tests: str, timeout: int = 30, debug: bool = False) -> float:
    """æ‰§è¡Œä»£ç å’Œæµ‹è¯•ï¼Œè¿”å›pass/failåˆ†æ•°"""
    if not code or not tests:
        return 0.0
    
    # æ¸…ç†ä»£ç ï¼šç§»é™¤å¯èƒ½åŒ…å«çš„é”™è¯¯ä¿¡æ¯
    # å¦‚æœä»£ç ä¸­åŒ…å« Tracebackï¼Œåªä¿ç•™ Traceback ä¹‹å‰çš„éƒ¨åˆ†
    if "Traceback" in code:
        code = code.split("Traceback")[0].strip()
    if "Error:" in code and "def " in code:
        # å¦‚æœ Error: åœ¨å‡½æ•°å®šä¹‰ä¹‹åï¼Œä¿ç•™å‡½æ•°å®šä¹‰ä¹‹å‰çš„éƒ¨åˆ†
        error_pos = code.find("Error:")
        def_pos = code.rfind("def ", 0, error_pos)
        if def_pos >= 0:
            code = code[:def_pos] + code[def_pos:].split("Error:")[0].strip()
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
        # åˆå¹¶ä»£ç å’Œæµ‹è¯•
        full_code = code + "\n\n" + tests
        f.write(full_code)
        temp_path = f.name
    
    try:
        result = subprocess.run(
            ["python", temp_path],
            capture_output=True,
            text=True,
            timeout=timeout
        )
        if result.returncode == 0:
            return 1.0
        else:
            # æ‰§è¡Œå¤±è´¥ï¼Œè®°å½•é”™è¯¯ä¿¡æ¯ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹ï¼‰
            if debug:
                print(f"   æ‰§è¡Œé”™è¯¯ (returncode={result.returncode}):")
                if result.stderr:
                    print(f"   stderr: {result.stderr[:500]}")
                if result.stdout:
                    print(f"   stdout: {result.stdout[:500]}")
            return 0.0
    except subprocess.TimeoutExpired:
        if debug:
            print(f"   æ‰§è¡Œè¶…æ—¶ (>{timeout}s)")
        return 0.0
    except Exception as e:
        if debug:
            print(f"   æ‰§è¡Œå¼‚å¸¸: {e}")
        return 0.0
    finally:
        import os
        try:
            os.unlink(temp_path)
        except:
            pass


# Import unified render_state function - MUST be identical to training
# (Already has PROJECT_ROOT in sys.path from earlier)
from policy.render_state import render_state
from policy.infer import select_action, execute_action


def generate_response(model, tokenizer, prompt: str, max_length: int = 2048) -> str:
    """ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå“åº”"""
    # å¯¹äºInstructæ¨¡å‹ï¼Œä½¿ç”¨chat template
    if hasattr(tokenizer, 'apply_chat_template') and tokenizer.chat_template:
        messages = [{"role": "user", "content": prompt}]
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=max_length)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.3,
            top_p=0.9,
            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
        )
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # ç§»é™¤promptéƒ¨åˆ†
    if prompt in generated_text:
        response = generated_text[len(prompt):].strip()
    else:
        response = generated_text.strip()
    return response


def extract_action_from_response(response: str, state: Dict) -> str:
    """ä»å“åº”ä¸­æå–action (LOW/MID/HIGH)"""
    response_lower = response.lower()
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜ç¡®çš„actionæ ‡è®°
    if "action:" in response_lower or "proactivity:" in response_lower:
        for action in ["LOW", "MID", "HIGH"]:
            if action in response.upper():
                return action
    
    # åŸºäºå†…å®¹æ¨æ–­
    question_count = response.count("?")
    if question_count >= 2:
        return "HIGH"
    elif question_count == 1:
        return "MID"
    elif "code" in response_lower or "solution" in response_lower or "```" in response:
        return "LOW"
    else:
        return "MID"  # é»˜è®¤


def evaluate_model(
    model_dir: str,
    base_model: str,
    prefs_path: str,
    max_samples: Optional[int] = None,
    output_path: Optional[str] = None
):
    """è¯„ä¼°DPOæ¨¡å‹"""
    print(f"ğŸ“Š åŠ è½½æ¨¡å‹: {model_dir}")
    print(f"ğŸ“Š Baseæ¨¡å‹: {base_model}")
    
    # Scheme A: Separated Architecture
    # Policy model only predicts action, code generation is separate
    print("ğŸ“‹ ä½¿ç”¨åˆ†ç¦»æ¶æ„ (Scheme A)")
    print("   - Policyæ¨¡å‹: é¢„æµ‹action (LOW/MID/HIGH)")
    print("   - Codeç”Ÿæˆ: ä½¿ç”¨ç‹¬ç«‹æ¨¡å‹ï¼ˆä¸å—DPOå½±å“ï¼‰")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    prefs = load_jsonl(Path(prefs_path))
    if max_samples:
        prefs = prefs[:max_samples]
    
    print(f"ğŸ“Š è¯„ä¼° {len(prefs)} ä¸ªæ ·æœ¬", flush=True)
    
    results = []
    task_success_count = 0
    total_samples = 0
    
    # Code generation strategy:
    # 1. If OpenAI API is available, use it (higher quality)
    # 2. Otherwise, use base Llama model (no API needed)
    use_openai = os.environ.get("OPENAI_API_KEY") is not None
    if use_openai:
        print("âœ… ä½¿ç”¨OpenAI APIè¿›è¡Œä»£ç ç”Ÿæˆ")
        code_model_name = None
    else:
        print("âœ… ä½¿ç”¨Base Llamaæ¨¡å‹è¿›è¡Œä»£ç ç”Ÿæˆï¼ˆä¸éœ€è¦APIï¼‰")
        code_model_name = base_model  # Use base Llama model for code generation
    
    for i, pref in enumerate(prefs):
        state = pref["state"]
        
        # Scheme A: Separated Architecture
        # Step 1: Predict action using policy model
        state_text = render_state(state)
        predicted_action = select_action(state_text, model_dir, base_model)
        
        # Step 2: Generate code using separate code generation
        task_prompt = state.get("query", "")
        domain = state.get("domain", "coding")
        
        response = execute_action(
            predicted_action,
            task_prompt,
            domain,
            code_model_name=code_model_name,  # Use base Llama model if no API
            use_openai=use_openai
        )
        
        # æå–ä»£ç ï¼ˆå¦‚æœæ˜¯codingä»»åŠ¡ï¼‰
        code = None
        if state["domain"] == "coding":
            code = extract_code_from_text(response)
            # è°ƒè¯•ä¿¡æ¯ï¼šè®°å½•ä»£ç æå–æƒ…å†µ
            if not code and (i < 3 or (i + 1) % 20 == 0):
                # æ ¹æ®predicted_actionåˆ¤æ–­ï¼šHIGH actionæ˜¯é—®é—®é¢˜ï¼Œè¿™æ˜¯æ­£å¸¸çš„
                if predicted_action == "HIGH":
                    print(f"\nğŸ“‹ æ ·æœ¬ {i+1}: é¢„æµ‹HIGH actionï¼ˆé—®é—®é¢˜ï¼‰")
                    print(f"   å“åº”ç±»å‹: æ¾„æ¸…é—®é¢˜ï¼ˆæ­£å¸¸è¡Œä¸ºï¼‰")
                    print(f"   å“åº”é¢„è§ˆ: {response[:300]}...")
                elif predicted_action == "MID":
                    print(f"\nğŸ“‹ æ ·æœ¬ {i+1}: é¢„æµ‹MID actionï¼ˆé—®ä¸€ä¸ªé—®é¢˜ï¼‰")
                    print(f"   å“åº”ç±»å‹: æ¾„æ¸…é—®é¢˜ï¼ˆæ­£å¸¸è¡Œä¸ºï¼‰")
                    print(f"   å“åº”é¢„è§ˆ: {response[:300]}...")
                else:
                    # LOW actionåº”è¯¥ç”Ÿæˆä»£ç ï¼Œå¦‚æœæ²¡æœ‰ä»£ç æ‰æ˜¯é—®é¢˜
                    print(f"\nâš ï¸  æ ·æœ¬ {i+1}: LOW actionä½†æœªæå–åˆ°ä»£ç ")
                    print(f"   å“åº”é•¿åº¦: {len(response)}")
                    print(f"   å“åº”é¢„è§ˆ: {response[:500]}...")
            elif code and i < 3:
                # å¯¹å‰3ä¸ªæ ·æœ¬ï¼Œæ˜¾ç¤ºå®Œæ•´å“åº”ä»¥ä¾¿è°ƒè¯•
                print(f"\nğŸ“ æ ·æœ¬ {i+1} å®Œæ•´å“åº”:")
                print("="*80)
                print(response)
                print("="*80)
                print(f"\nğŸ“¦ æå–çš„ä»£ç :")
                print("="*80)
                print(code)
                print("="*80)
        
        # è®¡ç®—task score
        task_score = 0.0
        if state["domain"] == "coding" and code:
            tests = state.get("convcodeworld_tests")
            if tests:
                task_score = score_code_passfail(code, tests, debug=(i < 3))
                if task_score > 0:
                    task_success_count += 1
                elif i < 3 or (i + 1) % 20 == 0:
                    print(f"\nâš ï¸  æ ·æœ¬ {i+1}: ä»£ç æ‰§è¡Œå¤±è´¥ (score=0)")
                    print(f"   æå–çš„ä»£ç é•¿åº¦: {len(code)}")
                    print(f"   ä»£ç é¢„è§ˆ: {code[:300]}...")
                    # æ˜¾ç¤ºå®Œæ•´ä»£ç ï¼ˆå‰3ä¸ªæ ·æœ¬ï¼‰
                    if i < 3:
                        print(f"   å®Œæ•´ä»£ç :\n{code}")
                        print(f"   æµ‹è¯•ç”¨ä¾‹é•¿åº¦: {len(tests)}")
                total_samples += 1
        elif state["domain"] == "coding" and not code:
            # è®°å½•æ²¡æœ‰æå–åˆ°ä»£ç çš„æƒ…å†µ
            if i < 3 or (i + 1) % 20 == 0:
                # æ ¹æ®predicted_actionåˆ¤æ–­ï¼šHIGH/MID actionæ˜¯é—®é—®é¢˜ï¼Œè¿™æ˜¯æ­£å¸¸çš„
                if predicted_action in ["HIGH", "MID"]:
                    print(f"\nğŸ“‹ æ ·æœ¬ {i+1}: é¢„æµ‹{predicted_action} actionï¼ˆé—®é—®é¢˜ï¼‰")
                    print(f"   å“åº”ç±»å‹: æ¾„æ¸…é—®é¢˜ï¼ˆæ­£å¸¸è¡Œä¸ºï¼Œtask_score=0ï¼‰")
                    print(f"   å“åº”é¢„è§ˆ: {response[:300]}...")
                else:
                    # LOW actionåº”è¯¥ç”Ÿæˆä»£ç 
                    print(f"\nâš ï¸  æ ·æœ¬ {i+1}: LOW actionä½†æœªæå–åˆ°ä»£ç ")
                    print(f"   å“åº”é•¿åº¦: {len(response)}")
                    print(f"   å“åº”é¢„è§ˆ: {response[:300]}...")
        
        # è®¡ç®—interrupt costï¼ˆç®€åŒ–ç‰ˆï¼‰
        n_questions = response.count("?")
        length_tokens = len(response.split())
        meta = {"reject_signal": 0, "off_topic": 0}
        interrupt_cost = compute_interrupt_cost(meta, n_questions, length_tokens, 0)
        
        # æ€»reward
        total_r = total_reward(task_score, interrupt_cost)
        
        results.append({
            "state_id": state.get("id", f"sample_{i}"),
            "predicted_action": predicted_action,
            "chosen_action": pref.get("chosen_action", "MID"),
            "task_score": task_score,
            "interrupt_cost": interrupt_cost,
            "total_reward": total_r,
            "response_length": len(response),
            "n_questions": n_questions,
        })
        
        if (i + 1) % 10 == 0:
            print(f"  å¤„ç†è¿›åº¦: {i+1}/{len(prefs)}", flush=True)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    task_success_rate = (task_success_count / total_samples * 100) if total_samples > 0 else 0.0
    avg_reward = sum(r["total_reward"] for r in results) / len(results) if results else 0.0
    avg_task_score = sum(r["task_score"] for r in results) / len(results) if results else 0.0
    
    # Actionå‡†ç¡®ç‡
    action_matches = sum(1 for r in results if r["predicted_action"] == r["chosen_action"])
    action_accuracy = (action_matches / len(results) * 100) if results else 0.0
    
    summary = {
        "task_success_rate": task_success_rate,
        "avg_reward": avg_reward,
        "avg_task_score": avg_task_score,
        "action_accuracy": action_accuracy,
        "total_samples": len(results),
        "task_evaluated_samples": total_samples,
        "task_success_count": task_success_count,
    }
    
    print("\n" + "="*50)
    print("ğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"  Task Success Rate: {task_success_rate:.2f}%")
    print(f"  Average Reward: {avg_reward:.4f}")
    print(f"  Average Task Score: {avg_task_score:.4f}")
    print(f"  Action Accuracy: {action_accuracy:.2f}%")
    print("="*50)
    
    # ä¿å­˜ç»“æœ
    if output_path:
        output = {
            "summary": summary,
            "detailed_results": results,
        }
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    return summary


def main():
    parser = argparse.ArgumentParser(description="è¯„ä¼°DPOæ¨¡å‹")
    parser.add_argument("--model_dir", type=str, required=True, help="è®­ç»ƒå¥½çš„æ¨¡å‹ç›®å½•")
    parser.add_argument("--base_model", type=str, required=True, help="Baseæ¨¡å‹åç§°")
    parser.add_argument("--prefs", type=str, required=True, help="Preference pairsæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--max_samples", type=int, default=None, help="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°")
    parser.add_argument("--output", type=str, default=None, help="è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    evaluate_model(
        model_dir=args.model_dir,
        base_model=args.base_model,
        prefs_path=args.prefs,
        max_samples=args.max_samples,
        output_path=args.output,
    )


if __name__ == "__main__":
    main()


