# Data Directory Structure

Data files organized by pipeline stage.

## Folders

- **`external/`**: Raw, unmodified datasets (read-only, never modify in place)
  - Example: `MBPP/mbpp_raw_*.jsonl` (original MBPP splits)

- **`seeds/`**: Processed seed tasks (input to trajectory generation)
  - Example: `mbpp_states.jsonl` (converted from MBPP, contains `{query, dialogue_turn, query_clarity, task_complexity, prev_reject, mbpp_tests}`)
  - Generated by: `scripts/convert_mbpp_to_states.py`

- **`logs/`**: Generated trajectories (Step 1 output)
  - Format: `traj_*.jsonl` (one trajectory per line: `{state, action, assistant_msg, persona, user_reaction}`)
  - Generated by: `scripts/generate_trajectories.py`
  - Used by: `reward/compute_rewards.py --trajectories`
  - **Can be reused** to recompute rewards with different weights

- **`dpo/`**: Preference pairs (Step 2 output)
  - Format: `prefs_*.jsonl` (one pair per line: `{state, chosen_action, rejected_action, rewards, task_scores, interrupt_costs}`)
  - Generated by: `reward/compute_rewards.py`
  - Used by: `policy/train_dpo.py`

- **`eval/`**: Evaluation results (metrics, figures)
  - Example: `results.json`, `pareto.png`

## Pipeline Flow

### Step 0: Dataset Conversion
- **Input**: `external/MBPP/mbpp_raw_*.jsonl` (raw MBPP dataset)
- **Script**: `scripts/convert_mbpp_to_states.py`
- **Output**: `seeds/mbpp_states.jsonl` (converted to our state format)
- **Process**: Extracts `{query, dialogue_turn, query_clarity, task_complexity, prev_reject, mbpp_tests}` from raw MBPP

### Step 1: Generate Trajectories
- **Input**: `seeds/mbpp_states.jsonl` (or synthetic states)
- **Script**: `scripts/generate_trajectories.py`
  - Loads behavior templates from `prompts/coding_*.txt` or `prompts/planning_*.txt`
  - Generates assistant output (via `llm/provider.py` if `--llm_model` provided, else dummy)
  - Gets user reaction via `simulator/simulate.py`
- **Output**: `logs/traj_*.jsonl` (trajectories: state + action + assistant_msg + persona + user_reaction)
- **Process**: For each state, generates 3 trajectories (LOW/MID/HIGH actions)

### Step 2: Compute Rewards
- **Input**: `logs/traj_*.jsonl` (trajectories from Step 1)
- **Script**: `reward/compute_rewards.py`
  - Computes `R_task` via `reward/compute.py` (can use `reward/mbpp_eval.py` for MBPP tests)
  - Computes `C_interrupt` from simulator meta (questions, rejects, length, off_topic)
  - Calculates `R = R_task − α·C_interrupt`
- **Output**: `dpo/prefs_*.jsonl` (preference pairs: chosen vs rejected actions with scores)
- **Process**: Ranks 3 actions by reward, picks best as chosen and worst as rejected

### Step 3: Train Policy
- **Input**: `dpo/prefs_*.jsonl` (preference pairs)
- **Script**: `policy/train_dpo.py`
- **Output**: Trained model (saved to `outputs/` or similar)
- **Process**: DPO training to learn action selection policy π(a|s)

### Step 4: Evaluate
- **Input**: Trained model + test data
- **Script**: `eval/evaluate.py`, `eval/plot_pareto.py`
- **Output**: `eval/results.json`, `eval/pareto.png` (metrics and visualizations)

### Complete Flow Diagram
```
external/MBPP/mbpp_raw_*.jsonl
    ↓ [convert_mbpp_to_states.py]
seeds/mbpp_states.jsonl
    ↓ [generate_trajectories.py + prompts/ + llm/provider.py + simulator/]
logs/traj_*.jsonl
    ↓ [compute_rewards.py + reward/compute.py + reward/mbpp_eval.py]
dpo/prefs_*.jsonl
    ↓ [train_dpo.py]
[trained model]
    ↓ [evaluate.py, plot_pareto.py]
eval/results.json, eval/pareto.png
```

**Note**: `logs/`, `dpo/`, and `eval/` are git-ignored (generated files).
