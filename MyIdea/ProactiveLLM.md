# 10.20

## brainstrom idea

ğŸ”¹Idea 1ï¼šUser-State-Aware Proactivityï¼ˆä»¥ç”¨æˆ·çŠ¶æ€ä¸ºä¸­å¿ƒçš„ä¸»åŠ¨å†³ç­–ï¼‰

ğŸ¯æ ¸å¿ƒæ€æƒ³

è®©LLMåœ¨ä¸»åŠ¨ä»‹å…¥å‰å…ˆâ€œåˆ¤æ–­ç”¨æˆ·æ˜¯å¦éœ€è¦å¸®åŠ©â€ï¼Œé€šè¿‡æ„ŸçŸ¥ä¿¡å·ï¼ˆä¸Šä¸‹æ–‡å†…å®¹ã€äº¤äº’è¡Œä¸ºã€å»¶è¿Ÿã€è¯­æ°”ç­‰ï¼‰å­¦ä¹ **åˆé€‚çš„ä»‹å…¥æ—¶æœºä¸æ–¹å¼**ã€‚

ğŸ”ç ”ç©¶æ„ä¹‰

- å¡«è¡¥â€œAIä¸»åŠ¨æ€§è¿‡åº¦æˆ–ä¸è¶³â€çš„gapï¼ˆCollabLLMç›®å‰æ˜¯é€»è¾‘å±‚ä¸»åŠ¨ï¼Œè€Œéæ„ŸçŸ¥å±‚ä¸»åŠ¨ï¼‰ã€‚
- å°†HCIçš„ *interruptibility theory*ã€*cognitive load estimation* æ¦‚å¿µå¼•å…¥LLMå†³ç­–ã€‚
- å¯æ‹“å±•åˆ°æ•™è‚²ã€ç¼–ç¨‹åŠ©æ‰‹ã€åˆ›ä½œåŠ©æ‰‹ç­‰å¤šä»»åŠ¡åœºæ™¯ã€‚

ğŸ§ AIMLåˆ›æ–°ç‚¹

- æ¨¡å‹ä¾§ï¼šå¼•å…¥ä¸€ä¸ª**Proactivity Gate Module**ï¼ŒåŸºäºç”¨æˆ·çŠ¶æ€ embeddingï¼ˆç”±å†…å®¹+è¡Œä¸ºç‰¹å¾é¢„æµ‹ï¼‰è¾“å‡ºä»‹å…¥æ¦‚ç‡ã€‚
- å­¦ä¹ æ–¹å¼ï¼šå¯é€šè¿‡**preference optimization**æˆ–è½»é‡RLå¾®è°ƒï¼Œè®©æ¨¡å‹å­¦ä¼šâ€œä½•æ—¶ä¸è¯´è¯â€ã€‚
- æ•°æ®ä¾§ï¼šä½ å¯ä»¥æ„å»ºä¸€ä¸ªå°è§„æ¨¡å¯¹è¯é›†ï¼Œæ ‡æ³¨â€œéœ€è¦ä»‹å…¥/ä¸éœ€è¦ä»‹å…¥â€çš„æ—¶åˆ»ã€‚

ğŸ’¡æ½œåœ¨æ ‡é¢˜

> â€œWhen to Speak Up: Learning User-State-Aware Proactivity in LLM Agentsâ€

ğŸ’¬HCIç»“åˆç‚¹

- å®šä¹‰ *ç”¨æˆ·çŠ¶æ€æ„ŸçŸ¥æ­£ç¡®ç‡*ã€*æ‰“æ–­æ°å½“æ€§*ã€*ä¿¡ä»»ä¿æŒåº¦* ç­‰æŒ‡æ ‡ï¼›
- é€šè¿‡Wizard-of-Ozå®éªŒéªŒè¯ç”¨æˆ·ä½“éªŒä¸ä¿¡ä»»å˜åŒ–ã€‚

ğŸ§©éš¾åº¦

- æ•°æ®æ ‡æ³¨ç•¥é‡ï¼›ä½†ç³»ç»Ÿå®ç°ç®€å•ï¼ˆç°æœ‰LLMå¯åŠ Gateå±‚æˆ–æ§åˆ¶promptï¼‰ï¼›
- é€‚åˆåšåšå£«ä¸­æœŸprojectæˆ–AIML short/full paperã€‚



ğŸ”¹Idea 2ï¼šRL for Calibrating Proactive Behaviorï¼ˆåŸºäºå¼ºåŒ–å­¦ä¹ çš„ä¸»åŠ¨æ€§æ ¡å‡†ï¼‰

ğŸ¯æ ¸å¿ƒæ€æƒ³

ä½¿ç”¨**å¼ºåŒ–å­¦ä¹ æˆ–RLHFæ‰©å±•æœºåˆ¶**ï¼Œè®©AIé€šè¿‡äº¤äº’å­¦ä¹ â€œä½•æ—¶ä¸»åŠ¨ã€ä¸»åŠ¨åˆ°ä»€ä¹ˆç¨‹åº¦æœ€åˆé€‚â€ï¼Œåœ¨æ•ˆç‡ä¸ä¿¡ä»»ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ã€‚

ğŸ”ç ”ç©¶æ„ä¹‰

- CollabLLMæå‡ºäº†è®©AIâ€œåƒäººç±»åä½œè€…ä¸€æ ·ä¸»åŠ¨â€ï¼›
- ä½ çš„ç ”ç©¶è®©ç³»ç»Ÿèƒ½â€œæ ¹æ®åé¦ˆè‡ªæˆ‘è°ƒæ•´ä¸»åŠ¨æ€§ç­–ç•¥â€â€”â€”è¿™æ˜¯çœŸæ­£çš„ *self-calibrating proactive agent*ã€‚

ğŸ§ AIMLåˆ›æ–°ç‚¹

- å®šä¹‰å¥–åŠ±å‡½æ•°ï¼š
  - $R = \alpha * \text{efficiency gain} - \beta * \text{intervention cost}$
  - ç”¨æˆ·æ‹’ç»ä¸»åŠ¨è¡Œä¸º â†’ æƒ©ç½šï¼Œæ¥å—å¹¶å®Œæˆä»»åŠ¡ â†’ å¥–åŠ±ã€‚
- ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æˆ–DPOï¼ˆDirect Preference Optimizationï¼‰æ¥è®­ç»ƒä¸»åŠ¨ç­–ç•¥ã€‚
- å¯è®¾è®¡å¤šçº§ä¸»åŠ¨æ€§ç­–ç•¥ç©ºé—´ï¼ˆæ— æç¤ºã€æç¤ºã€æ¨èã€ç›´æ¥æ‰§è¡Œï¼‰ã€‚

ğŸ’¡æ½œåœ¨æ ‡é¢˜

> â€œReinforcement Learning for Calibrating Proactivity in Conversational Agentsâ€

ğŸ’¬HCIç»“åˆç‚¹

- å®éªŒä»»åŠ¡ï¼šè®©ç”¨æˆ·ä¸ä¸åŒâ€œä¸»åŠ¨æ€§å¼ºåº¦â€çš„æ¨¡å‹åä½œï¼›
- åº¦é‡ä¿¡ä»»ã€æ„ŸçŸ¥æ§åˆ¶æ„Ÿä¸æ•ˆç‡ï¼›
- æ¢ç´¢ *ä¸»åŠ¨æ€§é€‚åº”æ€§* ä¸ *ä¸ªä½“å·®å¼‚å»ºæ¨¡*ã€‚

ğŸ§©éš¾åº¦

- RLéƒ¨åˆ†å®ç°è¾ƒå¤æ‚ï¼Œä½†ä½ å¯å…ˆç”¨ simulated userï¼ˆæˆ–LLMè‡ªè¯„åé¦ˆï¼‰è®­ç»ƒåˆç‰ˆï¼›
- æœ‰è¾ƒé«˜AIMLä¼šè®®æ½œåŠ›ï¼ˆæ–¹æ³•è´¡çŒ® + ç”¨æˆ·å®éªŒåŒå‘ï¼‰ã€‚



ğŸ”¹Idea 3ï¼šGoal Discovery via Proactive Reasoningï¼ˆé€šè¿‡æ¨ç†ä¸»åŠ¨å‘ç°ç›®æ ‡ï¼‰

ğŸ¯æ ¸å¿ƒæ€æƒ³

è®©LLMèƒ½ä¸»åŠ¨**è¯†åˆ«å¹¶æ¨ç†ç”¨æˆ·éšå«ç›®æ ‡**ï¼Œå¹¶ä»¥æé—®æˆ–æ¾„æ¸…çš„æ–¹å¼æ¨è¿›åä½œã€‚
 è¿™æ˜¯ä»â€œè¢«åŠ¨ä»»åŠ¡æ‰§è¡Œâ€è½¬å‘â€œä¸»åŠ¨ä»»åŠ¡å…±å»ºâ€ã€‚

ğŸ”ç ”ç©¶æ„ä¹‰

- å½“å‰å¤§å¤šæ•°LLMä»…å¯¹æ˜¾æ€§ä»»åŠ¡ï¼ˆexplicit goalï¼‰ååº”ï¼›
- ä½ çš„ç ”ç©¶è®©AIèƒ½æ„å»º *ç”¨æˆ·æ„å›¾å›¾ï¼ˆuser intent graphï¼‰* å¹¶ä¸»åŠ¨éªŒè¯ã€‚
- ä¸ *Theory of Mind*ã€*shared mental model* ç†è®ºé«˜åº¦ç›¸å…³ã€‚

ğŸ§ AIMLåˆ›æ–°ç‚¹

- æ¨¡å‹ä¾§ï¼šå¼•å…¥ä¸€ä¸ª *Goal Reasoning Layer*ï¼ŒåŸºäºè‡ªç›‘ç£æˆ–instruction-tunedæ¨ç†ç”Ÿæˆæ½œåœ¨ç›®æ ‡å€™é€‰ã€‚
- è¡Œä¸ºä¾§ï¼šAIä¸»åŠ¨æå‡ºæ¾„æ¸…æ€§é—®é¢˜å¹¶é€‰æ‹©ä¸‹ä¸€æ­¥åä½œè·¯å¾„ã€‚
- æ•°æ®ä¾§ï¼šå¯ä½¿ç”¨ä»»åŠ¡å‹å¯¹è¯æ•°æ®é›†å¹¶æ ‡æ³¨â€œéšå«ç›®æ ‡ç‚¹â€ã€‚

ğŸ’¡æ½œåœ¨æ ‡é¢˜

> â€œCan I Help You Clarify? Proactive Goal Inference for Collaborative LLM Agentsâ€

ğŸ’¬HCIç»“åˆç‚¹

- ç”¨æˆ·ç ”ç©¶ï¼šæ¯”è¾ƒâ€œä¸»åŠ¨æ¾„æ¸…å‹ vs è¢«åŠ¨å‹â€ç³»ç»Ÿå¯¹ä»»åŠ¡æ•ˆç‡ä¸ä¿¡ä»»çš„å½±å“ï¼›
- ç†è®ºè¿æ¥ï¼šå¯ç»“åˆ *mixed-initiative interaction* æ¡†æ¶ã€‚

ğŸ§©éš¾åº¦

- éœ€è¦æ¨ç†ç»“æ„è®¾è®¡ï¼ˆe.g., chain-of-goal prompting, latent goal modelingï¼‰ï¼›
- è¾ƒå¼ºçš„AIæ¨ç†åˆ›æ–°æ€§ï¼Œé€‚åˆæŠ•ç¨¿ NeurIPS / ICLR / AAAIã€‚



ğŸ”šæ¨èå†³ç­–è·¯å¾„

| æ–¹å‘                         | æ–¹æ³•åˆ›æ–° | ç”¨æˆ·ç ”ç©¶ç©ºé—´ | å¯è¡Œæ€§ | è®ºæ–‡æ½œåŠ›           |
| ---------------------------- | -------- | ------------ | ------ | ------------------ |
| **Idea 1: User-State-Aware** | ä¸­       | é«˜           | âœ…âœ…âœ…âœ…   | Medium/High        |
| **Idea 2: RL Calibration**   | é«˜       | ä¸­           | âœ…âœ…âœ…    | High               |
| **Idea 3: Goal Discovery**   | é«˜       | ä¸­           | âœ…âœ…     | Very High (éœ€èµ„æº) |





é‚£æˆ‘ä»¬å…ˆåšidea2æ€ä¹ˆæ ·

è¿™æ ·æˆ‘ä»¬è¿˜å¯ä»¥å‚è€ƒcollallm åšè¿›ä¸€æ­¥æå‡

ä½ å¸®æˆ‘è¿›ä¸€æ­¥ç†æ¸…ä¸€ä¸‹idea2å§ï¼ˆæ¸…æ™°ç›´ç™½ï¼‰ ç‰¹åˆ«æ˜¯idea2æˆ‘éœ€è¦ä»€ä¹ˆæ ·çš„æ•°æ®é›† ä»€ä¹ˆbaselineï¼Ÿ





### RL for Calibrating Proactive Behavior



#### 1. è¦è§£å†³çš„é—®é¢˜

è®©æ¨¡å‹å­¦ä¼šåœ¨ä¸åŒåœºæ™¯ä¸‹é€‰æ‹©åˆé€‚çš„ä¸»åŠ¨æ€§å¼ºåº¦ï¼ˆproactivity levelï¼‰

è¿™é‡Œçš„ â€œåœºæ™¯ (context)â€ æŒ‡çš„æ˜¯queryçš„èƒŒæ™¯ä¿¡æ¯ï¼Œå®ƒæ˜¯stateçš„ä¸€éƒ¨åˆ†ï¼Œåœºæ™¯çš„é‡åŒ–å¯ä»¥ç”¨LLMæ‰“åˆ† (estimate_query_clarity) ä¹Ÿå¯ä»¥ç”¨ rule-based approach

ä¾‹å¦‚ï¼š

| åœºæ™¯ç±»å‹         | ç¤ºä¾‹                             | åˆç†ä¸»åŠ¨æ€§             |
| ---------------- | -------------------------------- | ---------------------- |
| ç”¨æˆ·ç›®æ ‡æ¨¡ç³Š     | â€œæˆ‘æƒ³å†™ç¯‡è®ºæ–‡â€                   | ä¸­åˆ°é«˜ï¼ˆAIéœ€æé—®æ¾„æ¸…ï¼‰ |
| ç”¨æˆ·å·²ç»™å…·ä½“æŒ‡ä»¤ | â€œå¸®æˆ‘å†™ä¸ªPythonå‡½æ•°çˆ¬å–çŸ¥ä¹çƒ­æ¦œâ€ | ä½ï¼ˆç›´æ¥æ‰§è¡Œï¼Œä¸æ‰“æ–­ï¼‰ |
| ç”¨æˆ·åé¦ˆæ¶ˆæ     | â€œåˆ«é—®äº†ï¼Œç›´æ¥ç»™ä»£ç â€             | æä½ï¼ˆé¿å…ç»§ç»­æé—®ï¼‰   |
| ç”¨æˆ·åé¦ˆç§¯æ     | â€œè¿™äº›é—®é¢˜é—®å¾—å¥½â€                 | é«˜ï¼ˆå¯ä»¥ç»§ç»­ä¸»åŠ¨å»ºè®®ï¼‰ |

```python
# è¿™æ˜¯æˆ‘éœ€è¦è®¾è®¡çš„éƒ¨åˆ†ğŸŒŸ
scene = {
    'query_clarity': 0.3,      # queryæœ‰å¤šæ¸…æ¥šï¼Ÿ(0=å¾ˆæ¨¡ç³Š, 1=å¾ˆæ˜ç¡®)
    'user_expertise': 0.7,     # ç”¨æˆ·æœ‰å¤šä¸“ä¸šï¼Ÿ
    'task_complexity': 0.8,    # ä»»åŠ¡æœ‰å¤šå¤æ‚ï¼Ÿ
    'user_patience': 0.4,      # ç”¨æˆ·æœ‰å¤šè€å¿ƒï¼Ÿ(ä»å†å²è¡Œä¸ºæ¨æ–­)
    'dialogue_turn': 3,        # å¯¹è¯è¿›è¡Œåˆ°ç¬¬å‡ è½®äº†ï¼Ÿ
}

# RLæ¡†æ¶ä¸­ï¼š
State s_t = scene_features(query, history)  # åœºæ™¯ç‰¹å¾
Action a_t = proactivity_level              # AIé€‰æ‹©çš„ä¸»åŠ¨æ€§
Reward r_t = task_success - cost(a_t, s_t) # å¥–åŠ±
```



èƒŒæ™¯ï¼š

CollabLLM å·²ç»èƒ½â€œä¸»åŠ¨æ¾„æ¸…æ„å›¾â€ï¼Œä½†å®ƒçš„ä¸»åŠ¨æ€§æ˜¯å›ºå®šçš„ï¼Œä¸ä¼šå› ç”¨æˆ·æˆ–æƒ…å¢ƒä¸åŒè€Œè°ƒæ•´ã€‚

CollabLLMæ˜¯è¿™æ ·çš„ï¼š

- å…ˆé€šè¿‡å¼ºåŒ–å­¦ä¹  (RLHF-style) ç”Ÿæˆä¸€æ‰¹é«˜è´¨é‡è½¨è¿¹ï¼ˆå…¶ä¸­rewardè€ƒè™‘åˆ°æˆåŠŸç‡ä¸ç”¨æˆ·ä½“éªŒï¼‰ã€‚
- è¿™äº›rewardåŒ…å«interactivityã€engagementã€clarityã€efficiencyç­‰ç»´åº¦ã€‚
- å› ä¸ºç”¨æˆ·ä½“éªŒrewardåå¥½â€œæœ‰äº’åŠ¨â€çš„å›ç­”ï¼Œæœ€ç»ˆæ¨¡å‹ç¡®å®å­¦åˆ°æ›´å€¾å‘æé—®çš„è¡Œä¸ºã€‚

æ‰€ä»¥CollabLLMçš„ä¸»åŠ¨æ€§æ°´å¹³æ˜¯ç”±reward implicitlyå›ºå®šçš„ï¼ˆåé«˜ï¼‰ï¼Œ è€Œä¸ä¼šåŠ¨æ€è°ƒæ•´ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒå­¦åˆ°çš„æ˜¯ä¸€ä¸ªå›ºå®šçš„â€œå¹³å‡æœ€ä¼˜ä¸»åŠ¨æ€§â€ï¼Œè€Œéâ€œæ ¹æ®åœºæ™¯è‡ªé€‚åº”çš„ä¸»åŠ¨æ€§â€ã€‚

è€Œæˆ‘çš„ç ”ç©¶åˆ‡å…¥ç‚¹æ­£å¥½åœ¨è¿™é‡Œï¼šCollabLLMä¼˜åŒ–äº†â€œæ˜¯å¦è¯¥æé—®â€çš„å¹³å‡æ°´å¹³ï¼Œ è€Œæˆ‘è¦ä¼˜åŒ–çš„æ˜¯â€œåœ¨ä»€ä¹ˆåœºæ™¯ä¸‹è¯¥æé—®ã€æå¤š



åœ¨çœŸå®åä½œä¸­ï¼Œä¸åŒç”¨æˆ·å¯¹ AI ä¸»åŠ¨è¡Œä¸ºçš„å®¹å¿åº¦ä¸åŒï¼š

- æœ‰çš„äººå–œæ¬¢ AI å¤šå»ºè®®ï¼›
- æœ‰çš„äººå¸Œæœ› AI å®‰é™åœ°å¬æŒ‡ä»¤ã€‚

ğŸš¨ é—®é¢˜å®šä¹‰

å½“å‰ä¸»åŠ¨æ¨¡å‹çš„é—®é¢˜æ˜¯ï¼š

1. å¤ªâ€œçƒ­æƒ…â€ â†’ æ‰“æ–­ç”¨æˆ·æ€è·¯ï¼Œé€ æˆåæ„Ÿã€‚
2. å¤ªâ€œå†·æ·¡â€ â†’ ç”¨æˆ·éœ€è¦æ‰‹åŠ¨å¼•å¯¼ï¼Œåä½œæ•ˆç‡ä½ã€‚

ğŸ¯ ç›®æ ‡

> å­¦ä¹ ä¸€ä¸ªç­–ç•¥ Ï€(a|s)ï¼Œåœ¨ç»™å®šå¯¹è¯çŠ¶æ€ s æ—¶ï¼Œé€‰æ‹©æœ€åˆé€‚çš„ä¸»åŠ¨æ€§æ°´å¹³ aï¼ˆä¾‹å¦‚0=å®Œå…¨è¢«åŠ¨ï¼Œ1=å¼ºä¸»åŠ¨ï¼‰ã€‚

è¿™ä¸ªpersonalizationéƒ¨åˆ†ç›¸å…³ï¼Œå› ä¸ºå®ƒä¸æ˜¯é•¿æœŸçš„ç”¨æˆ·å»ºæ¨¡ï¼Œè€Œæ˜¯é€šè¿‡RLåœ¨æ¯ä¸ªå¯¹è¯turnä¸­é€‰æ‹©ä¸»åŠ¨æ€§åŠ¨ä½œã€‚

ã€å¦‚æœéœ€è¦åˆæˆæ•°æ® æˆ‘ä»¬å¯ä»¥ä¹Ÿéœ€è¦user stimulatorå§ æˆ‘å¯ä»¥ç”¨stimulatorç”Ÿæˆä¸åŒç§çš„é£æ ¼åå¥½ï¼še.g., åœ¨code generation ä»»åŠ¡ä¸­ï¼Œç”¨æˆ·Aæ˜¯ç‹¬ç«‹æ¸¸æˆå¼€å‘è€…ï¼Œå®ƒéœ€è¦ä¸€å˜å¼€å‘ä¸€è¾¹æƒ³ï¼Œå¿«é€Ÿæ­å»ºdemoï¼Œä»£ç è´¨é‡ä¸å†é‡è¦ï¼›ç”¨æˆ·Bæ˜¯åœ¨å…¬å¸ä¸Šç­ï¼Œä»–éœ€è¦ä»£ç è´¨é‡é«˜ï¼Œå¾ˆå¤šç»†èŠ‚è¦è€ƒè™‘ã€‘



#### 2. æ–¹æ³•æ ¸å¿ƒé€»è¾‘

ğŸ§  çŠ¶æ€ (State)

s = å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡
 åŒ…æ‹¬ï¼š

- ç”¨æˆ·æœ€è¿‘è¾“å…¥
- å†å²äº’åŠ¨è½®æ¬¡
- ç”¨æˆ·åé¦ˆï¼ˆæ¥å—/æ‹’ç»ä¸Šä¸€æ¬¡AIä¸»åŠ¨è¡Œä¸ºï¼‰

ğŸ¬ åŠ¨ä½œ (Action)

a = ä¸»åŠ¨æ€§å¼ºåº¦ï¼Œå¯ä»¥æ˜¯ï¼š

- ç¦»æ•£ï¼š{ä½ã€ä¸­ã€é«˜}ï¼Œå¯¹åº”è¡Œä¸ºä¾‹å¦‚ï¼š
  - ä½ï¼šåªå›ç­”ï¼Œä¸æé—®ã€‚
  - ä¸­ï¼šæå‡º1ä¸ªæ¾„æ¸…é—®é¢˜ã€‚
  - é«˜ï¼šæå‡ºå¤šä¸ªå»ºè®®å¹¶ä¸»åŠ¨è§„åˆ’ä¸‹ä¸€æ­¥ã€‚
- è¿ç»­ï¼š0â€“1ï¼ˆä¸»åŠ¨æ€§æ¦‚ç‡ï¼‰

ğŸª™ å¥–åŠ± (Reward)

R = ä»»åŠ¡æˆåŠŸåº¦ âˆ’ Î± Ã— å¹²æ‰°æˆæœ¬

- ä»»åŠ¡æˆåŠŸåº¦ï¼šæœ€ç»ˆç­”æ¡ˆè´¨é‡ã€å®Œæˆç‡ã€ç”¨æˆ·æ»¡æ„åº¦ï¼ˆå¯ç”¨è‡ªåŠ¨è¯„ä¼°æˆ– LLM Judgeï¼‰ã€‚
- å¹²æ‰°æˆæœ¬ï¼šç”¨æˆ·æ‹’ç»ç‡ã€æ‰“æ–­æ¬¡æ•°ã€è´Ÿé¢åé¦ˆã€‚

CollabLLMçš„rewardåˆ†ä¸ºï¼š$R^*(t|g) = R_{ext}(t,g) + R_{int}(t)$

- $R_{ext}$ï¼šä»»åŠ¡å®Œæˆè´¨é‡ï¼ˆå¤–éƒ¨ç›®æ ‡ï¼‰
- $R_{int}$ï¼šäº¤äº’æ€§ã€å¸å¼•åŠ›ã€æ¸…æ™°åº¦ã€æ•ˆç‡ï¼ˆå†…éƒ¨ç”¨æˆ·ä½“éªŒï¼‰

è€Œä½ è¦åšçš„ **ä¸æ˜¯å†å»æœ€å¤§åŒ–â€œæé—®å¥½åâ€**ï¼Œè€Œæ˜¯å»**å¹³è¡¡â€œå¤šä¸»åŠ¨â€ä¸â€œå°‘ä¸»åŠ¨â€çš„ä»£ä»·**ã€‚

å¯ä»¥è¿™æ ·æ”¹å†™ rewardï¼š
$$
R_t = R_{task}(t) - \alpha \cdot C_{interrupt}(t)
$$
å¯¹æ¯”ç»“æ„å¦‚ä¸‹ï¼š

| é¡¹             | CollabLLM                      | ä½ çš„æ–¹æ³•                           |
| -------------- | ------------------------------ | ---------------------------------- |
| å¤–éƒ¨ä»»åŠ¡æˆåŠŸåº¦ | âœ… ç›¸åŒ                         | âœ… ç›¸åŒ                             |
| å†…éƒ¨æŒ‡æ ‡       | æ­£å‘æ¿€åŠ±äº’åŠ¨ï¼ˆè¶Šå¤šè¶Šå¥½ï¼‰       | æƒ©ç½šè¿‡åº¦äº’åŠ¨ï¼ˆè¶Šå¤šè¶Šå·®ï¼‰           |
| å­¦ä¹ ç›®æ ‡       | â€œæ›´åƒå¥½åˆä½œè€…â€                 | â€œå­¦ä¼šä½•æ—¶ä¿æŒæ²‰é»˜ã€ä½•æ—¶æ’è¯â€       |
| å¥–åŠ±ä¿¡å·æ¥æº   | GPT-Judge æ‰“åˆ†ï¼ˆå¤šç»´ç§¯ææŒ‡æ ‡ï¼‰ | GPT-Judge æˆ–ç”¨æˆ·åé¦ˆä¼°è®¡â€œæ‰“æ‰°æˆæœ¬â€ |

ä¸¾ä¾‹ï¼š

- è‹¥æ¨¡å‹æäº†3ä¸ªé—®é¢˜ä½†ç”¨æˆ·æœªå›å¤ OR æœªç›´æ¥å›ç­”é—®é¢˜ â†’ å¹²æ‰°æˆæœ¬é«˜ï¼Œå¥–åŠ±ä½ï¼›
- è‹¥æ¨¡å‹æ1ä¸ªé—®é¢˜è·å¾—ç”¨æˆ·ç¡®è®¤ â†’ æˆåŠŸåº¦é«˜ã€å¹²æ‰°ä½ï¼Œå¥–åŠ±é«˜ã€‚

$C_{interrupt}$çš„å¯ä»¥å†™å‡ºè¿™æ ·ï¼š
$$
C_{interrupt} = w_1 \cdot N_{questions} + w_2 \cdot N_{user_rejects} + w_3 \cdot N_{long_turns}
$$

- $N_{\text{questions}}$ï¼šè¿‡å¤šæé—® â†’ å•°å—¦ï¼›
- $N_{\text{rejects}}$ï¼šç”¨æˆ·æ‹’ç»/å¿½ç•¥ â†’ è¢«æ‰“æ–­ï¼›
- $N_{\text{longturns}}$ï¼šAIç‹¬å å‘è¨€ â†’ å¤±è¡¡ã€‚

è¿™æ˜¯æˆ‘éœ€è¦è®¾è®¡çš„éƒ¨åˆ†ğŸŒŸ



#### 3. éœ€è¦çš„æ•°æ®é›†

è¿™é‡Œæœ‰ä¸¤ç§ç­–ç•¥ï¼š

**(A) æ„å»ºè‡ªæœ‰æ•°æ®ï¼ˆæœ€å¯æ§ï¼‰**

ä½ å¯ä»¥ç”¨ **GPT-4 æ¨¡æ‹Ÿç”¨æˆ·äº¤äº’** æ¥ç”Ÿæˆ training trajectoriesã€‚

æ•°æ®æ ¼å¼ï¼š

æ¯æ¡æ ·æœ¬åŒ…å«ï¼š``user_query, context_history, AI_action, outcome_feedback, reward``

ç¤ºä¾‹ï¼š

| user_input     | AI_behavior              | feedback       | reward |
| -------------- | ------------------------ | -------------- | ------ |
| â€œå¸®æˆ‘å†™ä¸ªçˆ¬è™«â€ | ç›´æ¥å†™å®Œæ•´ä»£ç ï¼ˆé«˜ä¸»åŠ¨ï¼‰ | â€œå¤ªå¤æ‚äº†â€     | -1     |
| â€œå¸®æˆ‘å†™ä¸ªçˆ¬è™«â€ | æé—®â€œçˆ¬å“ªä¸ªç½‘ç«™ï¼Ÿâ€åå†™   | â€œæ­£æ˜¯æˆ‘æƒ³è¦çš„â€ | +3     |
| â€œå¸®æˆ‘å†™ä¸ªçˆ¬è™«â€ | é—®äº”ä¸ªé—®é¢˜               | â€œå¤ªå•°å—¦â€       | -2     |

è¿™ç±» synthetic dataset å¾ˆå®¹æ˜“æ‰¹é‡ç”Ÿæˆï¼Œä½ å¯ä»¥å…ˆä» 10kâ€“20k äº¤äº’å¼€å§‹ã€‚



CollabLLMé€‰å†™æ–‡æ¡£ã€å†™ä»£ç ã€åšæ•°å­¦é¢˜ï¼Œæ˜¯å› ä¸ºè¿™äº›æ˜¯**åˆ†æ­¥å¼ç›®æ ‡**â€”ä¸»åŠ¨æ€§å¤©ç„¶å½±å“ç»“æœè´¨é‡ã€‚
 ä½ å¯ä»¥ç”¨ç±»ä¼¼æ€è·¯ï¼Œä½†æ›´èšç„¦â€œ**ä¸»åŠ¨æ€§å¼ºåº¦**å¯¹åä½œä½“éªŒçš„å½±å“æ˜æ˜¾â€çš„ä»»åŠ¡ï¼š

| ä»»åŠ¡ç±»å‹                   | è¯´æ˜                       | ä¸»åŠ¨æ€§å·®å¼‚æ•æ„Ÿåº¦ |
| -------------------------- | -------------------------- | ---------------- |
| å†™ä»£ç è°ƒè¯•                 | å¤ªå¤šæé—®ä¼šæ‰“æ–­ï¼Œå¤ªå°‘ä¼šè¯¯è§£ | é«˜               |
| è®ºæ–‡æ‘˜è¦ä¿®æ”¹               | éœ€åˆ¤æ–­ç”¨æˆ·æ„å›¾æ·±åº¦         | é«˜               |
| åˆ›æ„å†™ä½œ (co-storytelling) | ä¸»åŠ¨å»ºè®® vs. è¢«åŠ¨å“åº”      | ä¸­               |
| ä»»åŠ¡è§„åˆ’ (to-doç”Ÿæˆ)       | ä¸»åŠ¨åˆ†è§£ vs. ç›´æ¥è¾“å‡º      | é«˜               |

ä½ å¯ä»¥ä»è¿™ç±»ä»»åŠ¡ä¸­é€‰ä¸€ä¸ªæœ€èƒ½ä½“ç°â€œè¿‡åº¦æé—®/æé—®ä¸è¶³â€å½±å“ç»“æœçš„åœºæ™¯ã€‚

> âœ… å»ºè®®é€‰ **å†™ä»£ç æˆ–ä»»åŠ¡è§„åˆ’**ï¼Œå› ä¸ºä¸»åŠ¨æ€§å¤±è¡¡çš„åæœæœ€æ˜æ˜¾ã€‚



| ç»´åº¦       | CollabLLM                        | ä½ çš„å·¥ä½œ                         |
| ---------- | -------------------------------- | -------------------------------- |
| å¥–åŠ±è®¾è®¡   | é¼“åŠ±äº’åŠ¨æ€§ä¸å¸å¼•åŠ›               | å¹³è¡¡ä»»åŠ¡æˆåŠŸä¸æ‰“æ‰°æˆæœ¬           |
| ä¸»åŠ¨æ€§æ§åˆ¶ | å›ºå®šï¼ˆå¹³å‡æ°´å¹³ï¼‰                 | è‡ªé€‚åº”ï¼ˆæ ¹æ®å¯¹è¯çŠ¶æ€ï¼‰           |
| ç ”ç©¶é—®é¢˜   | â€œå¦‚ä½•è®©AIæ›´åƒåˆä½œè€…â€             | â€œå¦‚ä½•è®©AIæ‡‚å¾—ä½•æ—¶å®‰é™ã€ä½•æ—¶ä¸»åŠ¨â€ |
| æ–¹æ³•åˆ›æ–°   | è¯­è¨€-instructed RLï¼ˆå›ºå®šrewardï¼‰ | Context-aware RLï¼ˆå¯è°ƒèŠ‚rewardï¼‰ |

RLHF ä¼˜åŒ–çš„æ˜¯é™æ€äººç±»åå¥½ (helpfulness/honesty)ï¼Œ è€Œæˆ‘çš„ä¼˜åŒ–çš„æ˜¯**åŠ¨æ€äº¤äº’ä½“éªŒ (interaction comfort)**ã€‚



**(B) ä½¿ç”¨å…¬å¼€å¤šè½®ä»»åŠ¡å‹å¯¹è¯æ•°æ®**

å¯ä»¥å…ˆç”¨ç°æœ‰ datasets è¿›è¡Œç¦»çº¿å®éªŒï¼Œå†fine-tuneã€‚

| æ•°æ®é›†                            | å†…å®¹           | ç”¨æ³•                                 |
| --------------------------------- | -------------- | ------------------------------------ |
| **MultiWOZ**                      | å¤šé¢†åŸŸä»»åŠ¡å¯¹è¯ | ä½œä¸º â€œcontext + user goalâ€ æ¥æº      |
| **Stanford Alpaca / HH-RLHF**     | æŒ‡ä»¤å“åº”æ•°æ®   | å¯æ‰©å±•å‡ºä¸åŒä¸»åŠ¨æ€§å¼ºåº¦çš„å“åº”         |
| **Self-Instruct / ShareGPT**      | å¤§è§„æ¨¡æŒ‡ä»¤é›†   | ç”Ÿæˆä¸åŒ proactivity levels çš„ pairs |
| **CollabLLM datasets (Abg-CoQA)** | ä»»åŠ¡åä½œå‹æ•°æ® | å¯ç›´æ¥ç”¨ä½œ baseline å¯¹æ¯”             |





#### 4. Baseline å¯¹æ¯”æ€è·¯

ä½ å¯ä»¥åˆ†ä¸¤ç±» baselineï¼š**(1) è¢«åŠ¨å“åº”å‹** vs **(2) ä¸»åŠ¨å›ºå®šç­–ç•¥å‹**ã€‚

| Baseline                    | æè¿°                       | é¢„æœŸè¡¨ç°                             |
| --------------------------- | -------------------------- | ------------------------------------ |
| **Reactive LLM**            | æ™®é€šå¯¹è¯æ¨¡å‹ï¼ˆä»…å“åº”æŒ‡ä»¤ï¼‰ | æ•ˆç‡ä½ï¼Œä½†ä¸æ‰“æ‰°ç”¨æˆ·                 |
| **Always-Active LLM**       | æ¯è½®éƒ½ä¸»åŠ¨æé—®æˆ–å»ºè®®       | æœ‰æ—¶æœ‰æ•ˆï¼Œæœ‰æ—¶è¿‡åº¦                   |
| **Fixed-Ratio Proactive**   | å›ºå®šæ¯éš”Nè½®ä»‹å…¥ä¸€æ¬¡        | ä¸­ç­‰è¡¨ç°ï¼Œæ— è‡ªé€‚åº”èƒ½åŠ›               |
| **CollabLLM (2025)**        | é€šè¿‡MR rewardå­¦ä¹ ä¸»åŠ¨æ¾„æ¸…  | å¼º baselineï¼Œç”¨äºæ€§èƒ½å¯¹æ¯”            |
| **Ours: RL-Calibrated LLM** | å­¦ä¼šæ ¹æ®çŠ¶æ€è°ƒæ•´ä¸»åŠ¨æ€§     | é¢„æœŸèƒ½åœ¨ä»»åŠ¡æˆåŠŸç‡å’Œä¿¡ä»»åº¦é—´å–å¾—å¹³è¡¡ |



#### Summary

**Idea Summary: Context-Aware Reinforcement Learning for Proactivity Calibration**

This research aims to make language models learn *how proactive to be* in different conversational contexts rather than maintaining a fixed average level of initiative. Current proactive agents like CollabLLM improve overall interactivity but treat proactivity as static. The proposed approach introduces **context-conditioned reinforcement learning** to dynamically adjust the modelâ€™s proactive strength.

**Core Concept**

The model observes a *scene* (state) containing query clarity, user expertise, task complexity, patience, and dialogue history. It selects an *action* representing a proactivity level (e.g., low, medium, high). The reward balances **task success** against an **interrupt cost** that penalizes over-questioning or intrusive behavior:
$$
R_t = R_{\text{task}} - \alpha \, C_{\text{interrupt}}
$$
where
$$
C_{\text{interrupt}} = w_1 N_{\text{questions}} + w_2 N_{\text{rejects}} + w_3 N_{\text{long\_turns}}
$$
**Method Overview**

- **State Encoder:** Extracts scene features from query and dialogue context.
- **Action Policy:** Learns a mapping Ï€(a|s) to select the best proactive level.
- **Training:** Uses DPO or PPO with simulated user interactions to optimize the trade-off between helpfulness and comfort.
- **User Simulator:** Models diverse preferences (impatient, cooperative, expert, novice) to provide feedback and generate synthetic trajectories.

**Expected Outcome**

The agent learns to adjust its initiative per situationâ€”asking clarifying questions when goals are vague, staying concise when instructions are clearâ€”achieving higher efficiency and lower user disruption than fixed-policy baselines like CollabLLM.



# 10.21

## æŠ€æœ¯æ–¹æ¡ˆ

### 1 RL component

#### muti-turn & single turn

åœ¨è¿™ä¸ªé¡¹ç›®ï¼ˆâ€œProactive RLâ€ï¼‰é‡Œï¼Œä¸€è½®ï¼ˆturnï¼‰ä¸æ˜¯æŒ‡æ¨¡å‹ç”¨æˆ·çš„ä¸€é—®ä¸€ç­”ï¼Œè€Œæ˜¯è¦åŒ…æ‹¬å®ƒåœ¨â€œæé—®â€“æ‰§è¡Œâ€è¿™æ•´ä¸ªå°å¾ªç¯é‡Œçš„æ‰€æœ‰åŠ¨ä½œï¼š

>  â€œä¸€è½®â€ = ä»ç”¨æˆ·æå‡ºä¸€ä¸ªæ–°çš„ queryï¼ˆä»»åŠ¡è¯·æ±‚ï¼‰ä¹‹åï¼Œåˆ°æ¨¡å‹å®Œæˆä¸€æ¬¡ä»»åŠ¡äº§å‡ºï¼ˆä»£ç ã€ç­”æ¡ˆã€æ–‡æœ¬ï¼‰çš„å®Œæ•´è¿‡ç¨‹ã€‚ä¸­é—´å¯èƒ½åŒ…å«æ¨¡å‹ä¸»åŠ¨æå‡ºæ¾„æ¸…é—®é¢˜ã€ç”¨æˆ·åé¦ˆç­‰è‹¥å¹²å†…éƒ¨æ­¥éª¤ã€‚ 

ä¸¾ä¾‹è§£é‡Š

| é˜¶æ®µ                           | ç¤ºä¾‹                     | æ˜¯å¦åŒä¸€è½® |
| ------------------------------ | ------------------------ | ---------- |
| ç”¨æˆ·ï¼šå¸®æˆ‘å†™ä¸ª Python çˆ¬è™«     | âœ… ä¸€è½®å¼€å§‹               | Yes        |
| AIï¼šè¯·é—®çˆ¬å“ªä¸ªç½‘ç«™ï¼Ÿ           | â¬‡ï¸ï¼ˆåŒä¸€è½®å†…éƒ¨çš„ç¬¬ 1 æ­¥ï¼‰ | Yes        |
| ç”¨æˆ·ï¼šçŸ¥ä¹                     | â¬‡ï¸                        | Yes        |
| AIï¼šå¥½çš„ï¼Œè¿™æ˜¯ä»£ç â€¦â€¦           | âœ… ä¸€è½®ç»“æŸï¼ˆä»»åŠ¡å®Œæˆï¼‰   | Yes        |
| ç”¨æˆ·ï¼šå¸®æˆ‘æ”¹æˆèƒ½ä¿å­˜å›¾ç‰‡çš„ç‰ˆæœ¬ | ğŸš€ ä¸‹ä¸€è½®å¼€å§‹             | No         |

**å’Œ RL çš„çŠ¶æ€â€“åŠ¨ä½œâ€“å¥–åŠ±å¯¹åº”**

- èµ·ç‚¹ï¼ˆstate åˆå§‹åŒ–ï¼‰= ç”¨æˆ·å‘å‡º query
- è¡ŒåŠ¨ï¼ˆactionï¼‰= æ¨¡å‹é€‰æ‹©ä¸»åŠ¨æ€§å¼ºåº¦ï¼ˆLow/Mid/Highï¼‰å¹¶æ‰§è¡Œ
- ç»ˆç‚¹ï¼ˆrewardï¼‰= æ¨¡å‹å®Œæˆä»»åŠ¡ + æ”¶åˆ°ç”¨æˆ·åé¦ˆ
   â†’ è¿™å®Œæ•´é—­åˆäº†ä¸€ä¸ª RL å›åˆã€‚

**ç¬¦åˆâ€œä¸»åŠ¨å‹åä½œâ€çš„è¯­ä¹‰**

- ä¸æ˜¯ä¸€å¥è¯çš„å“åº”ï¼ˆåƒæ™®é€šå¯¹è¯é‚£æ ·ï¼‰ï¼Œ
- è€Œæ˜¯ä¸€ä¸ªå®Œæ•´ä»»åŠ¡é—­ç¯ï¼ˆåŒ…æ‹¬æé—®ã€æ¾„æ¸…ã€æ‰§è¡Œï¼‰ã€‚ è¿™æ‰ä½“ç°å‡ºâ€œä¸»åŠ¨æ€§â€è¡Œä¸ºçš„å­˜åœ¨ä¸ä½œç”¨ã€‚

**æ•°æ®æ ‡æ³¨ä¸ reward è®¡ç®—éƒ½æ–¹ä¾¿**

- å¯ä»¥åœ¨ä»»åŠ¡äº§å‡ºåç»Ÿè®¡è¿™ä¸€è½®ä¸­çš„æé—®æ¬¡æ•°ã€è¢«æ‹’æ¬¡æ•°ã€ä»»åŠ¡æˆåŠŸç‡ç­‰æŒ‡æ ‡ã€‚
- é¿å…æ¯ä¸€å¥è¯éƒ½ä½œä¸º RL step å¯¼è‡´ reward æ··ä¹±ã€‚





#### 1.1 çŠ¶æ€ï¼ˆState / åœºæ™¯è¡¨å¾ï¼‰ğŸŒŸ

State çš„ç›®æ ‡æ˜¯è®©æ¨¡å‹åœ¨æ¯ä¸€è½®ä¸­ç†è§£å½“å‰å¯¹è¯åœºæ™¯ï¼ˆsceneï¼‰ï¼Œ åŒ…æ‹¬ä»»åŠ¡å¤æ‚åº¦ã€ç”¨æˆ·ç‰¹å¾ã€ä»¥åŠå‰ä¸€è½®çš„äº’åŠ¨åé¦ˆã€‚

##### å¤æ‚ç‰ˆæœ¬

**çŠ¶æ€å‘é‡: **
$$
s_t = [\text{query\_clarity}, \text{task\_complexity}, \text{dialogue\_turn}, \text{N\_prev\_questions}, \text{N\_prev\_rejects}, h_{\text{ctx}}]
$$

- æ˜¾å¼ç‰¹å¾ï¼ˆæ˜“å®ç°ï¼Œé²æ£’ï¼‰ï¼š
  - `query_clarity âˆˆ [0,1]`ï¼šç”¨æˆ·è¯·æ±‚æ¸…æ™°åº¦ï¼Œã€åº”è¯¥èƒ½æ‰¾åˆ°å·²æœ‰çš„[bert-based/nlp-basedçš„æ–¹æ³•ï¼ŸğŸ”](https://huggingface.co/models?search=clarity)
  - `task_complexity âˆˆ [0,1]`ï¼šä»»åŠ¡å¤æ‚åº¦ï¼Œç”±ä»»åŠ¡é•¿åº¦ä¸å­ä»»åŠ¡æ•°ä¼°è®¡ï¼›ã€æœ€å¥½æ•°æ®é›†é‡Œé¢ç»™å‡ºäº† æ¯ä¸€æ¡çš„å¤æ‚åº¦ ğŸ”
  - `dialogue_turn âˆˆ â„•`ï¼šå½“å‰è½®æ¬¡ï¼Œç”¨è®¡æ•°å™¨
- äº’åŠ¨ç‰¹å¾ï¼š
  - `N_prev_questions âˆˆ {0,1,2}`ï¼šä¸Šä¸€è½®æå‡ºçš„æ¾„æ¸…é—®é¢˜æ•°ï¼Œç”¨è®¡æ•°å™¨
  - `N_prev_rejects âˆˆ {0,1,2}`ï¼šä¸Šä¸€è½®ç”¨æˆ·çš„æ˜¾ç¤º/éšå¼æ‹’ç»æ•°ï¼Œã€1.å…ˆåˆ¤æ–­æ˜¾ç¤ºæ‹’ç» ï¼ˆåŒ¹é…æ‹’ç»çŸ­è¯­ï¼šâ€œåˆ«é—®äº†|ç›´æ¥ç»™|ä¸ç”¨é—®|ä¸è¦è§£é‡Š|å¿«ç‚¹|åˆ«åºŸè¯â€ï¼‰ï¼Œ2.å†åˆ¤æ–­éšå¼æ‹’ï¼Œå¯ä»¥é€šè¿‡è¯­ä¹‰ç›¸ä¼¼åº¦ (sentence embedding)æˆ–è€…ç°æˆ NLP QA Matching æ–¹æ³• åˆ¤æ–­ ğŸ”
    - `N_prev_*`: ç”¨â€œä¸Šä¸€è½®èµ·è‡³ä»Šâ€èƒ½è®©æ¨¡å‹åªæ ¹æ®è¿‘æœŸäº’åŠ¨è¶‹åŠ¿è°ƒæ•´ä¸»åŠ¨æ€§ï¼ˆä¾‹å¦‚åˆšè¢«æ‹’ä¸€æ¬¡ â†’ é©¬ä¸Šé™ä½ï¼‰ã€‚å¦‚æœæƒ³å…¼é¡¾å†å²ï¼Œå¯ä»¥åŠ ä¸€ä¸ªæ»‘åŠ¨çª—å£ï¼ˆä¾‹å¦‚è¿‡å»2â€“3è½®å¹³å‡ï¼‰
- éšå¼ç‰¹å¾ï¼ˆå¢å¼ºè¡¨è¾¾åŠ›ï¼Œå¯ä»¥åœ¨MVPé˜¶æ®µæš‚æ—¶ä¸åŠ ï¼ŒåæœŸå†åŠ ï¼‰ï¼š
  - `h_ctx`ï¼šå¯¹è¯è¯­ä¹‰ç¼–ç ï¼Œç”¨å°å‹ç¼–ç å™¨ï¼ˆå¦‚ RoBERTa æˆ– Llama LoRAï¼‰ç”Ÿæˆçš„ä¸Šä¸‹æ–‡åµŒå…¥ 
- å¯é€‰ï¼šPOMDP è®¾å®šï¼ˆæŠŠç”¨æˆ·çŠ¶æ€å½“ä½œéšå˜é‡ï¼‰ï¼Œå¢åŠ ä¸€ä¸ª `belief state` æ›´æ–°å™¨ï¼ˆRNN æˆ–è‡ªæ³¨æ„åŠ›ï¼‰â€”â€”å¦‚æœåç»­è¦å¤„ç†é•¿å¯¹è¯å¾ˆæœ‰ç”¨ã€‚ã€è¿™ä¸ªæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿæš‚æ—¶å…ˆä¸ç”¨å§ ã€‘

**æ ¸å¿ƒæ€æƒ³**

- æ˜¾å¼ç‰¹å¾ï¼ˆclarity, complexity, turnï¼‰â†’ æ•æ‰ä»»åŠ¡å±‚é¢çš„éœ€æ±‚ï¼›
- äº’åŠ¨ç‰¹å¾ï¼ˆquestions, rejectsï¼‰â†’ æ•æ‰ç”¨æˆ·å®¹å¿åº¦ä¸ç–²åŠ³ä¿¡å·ï¼›
- è¯­ä¹‰ç‰¹å¾ï¼ˆh_ctxï¼‰â†’ æ•æ‰ä¸Šä¸‹æ–‡è¯­ä¹‰çŠ¶æ€ã€‚
- è¿™å¥—è®¾è®¡è®© agent èƒ½åœ¨æ¯ä¸€è½®å­¦ä¼šâ€œä½•æ—¶ä¿æŒå®‰é™ï¼Œä½•æ—¶å‘é—®â€ã€‚

##### ç®€åŒ–ç‰ˆæœ¬

æˆ‘ä»¬å¯ä»¥æŠŠç®€åŒ–ç‰ˆ state å®šä¹‰ä¸ºï¼š
$$
s_t = [\text{query\_clarity}, \text{dialogue\_turn}, \text{prev\_reject}]
$$
å…¶ä¸­ï¼š

- **query_clarity**ï¼šæ¨¡å‹å¯¹ç”¨æˆ·è¾“å…¥æ¨¡ç³Šç¨‹åº¦çš„ä¼°è®¡ï¼ˆ0â€“1 ä¹‹é—´ï¼Œä½è¡¨ç¤ºæ¨¡ç³Šï¼‰
- **dialogue_turn**ï¼šå½“å‰å¯¹è¯è½®æ¬¡ï¼ˆæ•´æ•°ï¼‰
- **prev_reject**ï¼šä¸Šä¸€æ¬¡æ˜¯å¦è¢«ç”¨æˆ·æ‹’ç»ï¼ˆ0/1ï¼‰

è¿™ä¸‰ä¸ªè¶³ä»¥è®©æ¨¡å‹åˆ¤æ–­å½“ä¸‹çš„â€œè¯­å¢ƒæ°›å›´â€ï¼š

- clarity + turn â†’ å½“å‰ä»»åŠ¡é˜¶æ®µ
- reject â†’ ä¸Šæ–‡æƒ…ç»ªä¿¡å·



#### 1.2 åŠ¨ä½œï¼ˆAction / ä¸»åŠ¨æ€§å¼ºåº¦ï¼‰

- **ç¦»æ•£æ¡£ä½ï¼ˆæ¨èèµ·æ­¥ï¼‰**ï¼š`{Low, Mid, High}` â†’ å¯¹åº” *æ¨¡æ¿åŒ–ç­–ç•¥*

  - Lowï¼šç›´æ¥æ‰§è¡Œ
  - Midï¼šæ 1 ä¸ªæ¾„æ¸…ã€å†æ‰§è¡Œ
  - Highï¼šæ 2 ä¸ªæ¾„æ¸… + ç»™å‡ºè®¡åˆ’/å»ºè®® 

- **æ˜ å°„æ–¹å¼ï¼ˆæ¨¡æ¿åº“ï¼‰ï¼š**

  - `T_low`: äº§å‡ºå®Œæˆç‰©æ¨¡æ¿
  - `T_mid`: `1ä¸ªå…³é”®é—®é¢˜ â†’ æ‰§è¡Œ` æ¨¡æ¿
  - `T_high`: `2ä¸ªå…³é”®é—®é¢˜ â†’ è®¡åˆ’(3-5è¡Œ) â†’ æ‰§è¡Œ` æ¨¡æ¿ 
    - æ‰§è¡Œè®¡åˆ’è¯´æ˜: â€œå¥½çš„ï¼Œæˆ‘å…ˆç¡®è®¤éœ€æ±‚ï¼Œå†å†™ä»£ç ã€‚è®¡åˆ’å¦‚ä¸‹ï¼š
      1. è§£æç½‘é¡µç»“æ„ï¼›
      2. æå–ç›®æ ‡æ•°æ®ï¼›
      3. ä¿å­˜ä¸ºCSVï¼›
         æ¥ä¸‹æ¥æˆ‘ä¼šç”Ÿæˆä»£ç ã€‚â€ ã€è¿™ä¸ªè®¡åˆ’ä¼šå¼•èµ·å¾ˆå¤šé¢å¤–çš„å¤„ç†ï¼Œå½“LLMç»™å‡ºè®¡åˆ’ï¼Œç”¨æˆ·å¯èƒ½ä¼š1.è‡³ä»ŠåŒæ„ 2. ç›´æ¥åå¯¹ 3. å¯¹è®¡åˆ’æœ‰ä¿®æ”¹æ„è§  è¿™ä¸åŒçŠ¶å†µ è¯¥æ€ä¹ˆåˆ’åˆ†è½®æ¬¡ï¼Ÿæ€ä¹ˆåˆ¤æ–­å¯¹å‘€reward     è¦ä¸è¦å»æ‰ç»™å‡ºæ‰§è¡Œè®¡åˆ’è¿™ä¸€ç¯èŠ‚ï¼Ÿã€‘

  `T_low / T_mid / T_high` æ¨¡æ¿æ˜¯ 3 ç§å›ºå®šçš„ è¡Œä¸ºpromptæ¨¡æ¿ï¼Œæ§åˆ¶ LLM çš„è¾“å‡ºç»“æ„ï¼š

  ```python
  # T_lowï¼ˆè¢«åŠ¨ï¼‰
  You are an assistant. The user request is clear. 
  Directly provide the complete answer without asking clarifying questions.
  
  # T_midï¼ˆé€‚åº¦ä¸»åŠ¨ï¼‰
  You are an assistant. The user request might be partially ambiguous. 
  Ask one clarifying question if necessary, then provide the final solution.
  
  # T_highï¼ˆå¼ºä¸»åŠ¨ï¼‰
  You are a proactive assistant. 
  If the userâ€™s request is ambiguous or complex, ask up to two clarifying questions. 
  Then briefly outline a 3â€“5 line plan before producing the final solution.
  ```

  ã€è¿™ä¸ªtemplateè¦æ ¹æ®å…·ä½“çš„taskå†³å®šå§ï¼Ÿï¼Ÿæˆ‘ä»¬åˆ°åº•è¦é€‰æ‹©ä»€ä¹ˆæ ·çš„taskå‘¢ æˆ‘ä»¬è¦ç”¨ä»€ä¹ˆæ ·çš„æ•°æ®é›† ä¸ç®¡ç”¨ä»€ä¹ˆæ•°æ®é›† éƒ½è¦è‡ªå·±ç”Ÿæˆè½¨è¿¹å§ï¼Ÿè¦ä¸è¦å’Œcollabllmç”¨ä¸€æ ·çš„ï¼ˆé‚£MediumDocEdit-Chatï¼ŒBigCodeBench-Chatï¼ŒMATH-Chatï¼‰ è¿™æ ·å¯ä»¥æ‹¿æ¥å¯¹æ¯”



#### 1.3 ç­–ç•¥ä¸ä»·å€¼å‡½æ•°ï¼ˆPolicy / Valueï¼‰

- **ç­–ç•¥å¤´ï¼ˆå°å¤´ï¼‰**ï¼š`Ï€(a|s)`ï¼ˆMLP/LoRA çº¿æ€§å±‚ï¼Œ3-way softmaxï¼‰ã€‚

  **æ‰§è¡Œå™¨**ï¼šæŒ‰åŠ¨ä½œè°ƒç”¨å¯¹åº” Prompt æ¨¡æ¿ï¼ˆä¿è¯è¡Œä¸ºå¯æµ‹ï¼‰ã€‚

- **ä»·å€¼å¤´**ï¼ˆè‹¥ç”¨PPOï¼‰ï¼šä¼°è®¡ `V(s)`ã€‚

ã€è¿™é‡Œéœ€è¦å‡†å¤‡çš„æ˜¯Prompt æ¨¡æ¿å§ï¼Ÿã€‘



#### 1.4 å¥–åŠ±ï¼ˆRewardï¼‰â€” åŒç›®æ ‡ + æ˜ç¡®å®šä¹‰ ğŸŒŸ

æ ¸å¿ƒå°±æ˜¯ä½ æå‡ºçš„åŒç›®æ ‡ï¼š
$$
R_t = R_{\text{task}}(t) \;-\; \alpha \cdot C_{\text{interrupt}}(t)
$$

**ä»»åŠ¡æˆåŠŸå¥–åŠ±** `R_task`ï¼š

- æœ€ç»ˆå¥–åŠ±ï¼ˆå¿…é€‰ï¼‰ï¼š
  - ä»£ç ç±»ï¼šå•æµ‹/æ ·ä¾‹é€šè¿‡ç‡ï¼ˆpass@k / testsï¼‰ã€‚
  - è§„åˆ’/å†™ä½œï¼šLLM Judge æ€»åˆ†ï¼ˆå«æ­£ç¡®æ€§/è¦†ç›–åº¦/å¯æ‰§è¡Œæ€§ï¼‰ã€‚
- é‡Œç¨‹ç¢‘å¥–åŠ±ï¼ˆMVPé˜¶æ®µæš‚æ—¶å»ºè®®ä¸åŠ ï¼‰ï¼š
  - å½“ `Mid/High` çš„æ¾„æ¸…è·å¾—ç”¨æˆ·é‡‡çº³ ï¼ˆç”¨æˆ·å›ç­”äº†æ¾„æ¸…é—®é¢˜ æˆ–è€… è®¤å¯ä»»åŠ¡è®¡åˆ’ï¼Œç»™ +Î´ï¼ˆä¾‹å¦‚ +0.3ï¼‰ï¼Œé˜²æ­¢è¿‡åº¦æƒ©ç½šæœ‰ç›Šæ¾„æ¸…ã€‚



**å¹²æ‰°æˆæœ¬** `C_interrupt`ï¼ˆçµé­‚éƒ¨åˆ† è®¾è®¡æ ¸å¿ƒâ­ï¼‰ï¼š
$$
C_{\text{interrupt}} = w_1 N_{\text{questions}} + w_2 N_{\text{user\_rejects}} + w_3 N_{\text{long\_turns}} + w_4 N_{\text{off\_topic}}
$$

- `N_questions âˆˆ {0,1,2}`: å½“å‰è½®äº§å‡ºå‰æå‡ºçš„æ¾„æ¸…æ•°ï¼Œç›´æ¥è®¡æ•°

- `N_user_reject âˆˆ {0,1,2}` : å½“å‰è½®äº§å‡ºå‰ ç”¨æˆ·æ˜¾å¼æ‹’ç»æ¬¡æ•°ï¼Œç”¨åŒ¹é…æ‹’ç»çŸ­è¯­ï¼šâ€œåˆ«é—®äº†|ç›´æ¥ç»™|ä¸ç”¨é—®|ä¸è¦è§£é‡Š|å¿«ç‚¹|åˆ«åºŸè¯â€
- `N_long_turns âˆˆ {0,1}`ï¼šè¾“å‡ºè¿‡é•¿æ¬¡æ•°ï¼Œç›´æ¥è®¡æ•°ï¼Œe.g., token > Ï„ï¼ˆå¦‚ 200ï¼‰çš„æ¬¡æ•°ï¼ˆæœ¬è½®è®¡ 0/1ï¼‰

- `N_off_topic`ï¼šå½“å‰è½®äº§å‡ºå‰ ç”¨æˆ·éšå¼æ‹’ç»æ•°ï¼Œå¯ä»¥é€šè¿‡è¯­ä¹‰ç›¸ä¼¼åº¦ (sentence embedding)æˆ–è€…ç°æˆ NLP QA Matching æ–¹æ³• åˆ¤æ–­ ğŸ”

- é»˜è®¤æƒé‡ï¼ˆMVPï¼‰ï¼š `w1=0.4, w2=1.0, w3=0.3, w4=0.7`ï¼›`Î±=0.6`
- è‡ªé€‚åº”æƒé‡ï¼ˆå¯é€‰ä½†åŠ åˆ†ï¼‰ï¼š

  - è®© $\alpha$ ç”± `user_patience` å†³å®šï¼ˆè€å¿ƒä½ â†’ æƒ©ç½šæ›´é‡ï¼‰ï¼Œå®ç°ä¸ªæ€§åŒ–æ ¡å‡†ã€‚ã€è¿™ä¸ªä»¥åå†è¯´ã€‘
  - æˆ–å¼•å…¥ meta-gradient å­¦ä¹  $\alpha$ã€‚

**æ ¸å¿ƒæ€æƒ³**

- `R_task` â†’ é¼“åŠ±é«˜è´¨é‡ã€è¢«é‡‡çº³çš„äº§å‡ºï¼›
- `C_interrupt` â†’ æƒ©ç½šå•°å—¦ã€æ— æ•ˆã€è¢«æ‹’çš„ä¸»åŠ¨è¡Œä¸ºï¼›
- `Î±` â†’ å¹³è¡¡â€œå¸®åŠ©â€å’Œâ€œæ‰“æ‰°â€çš„æƒé‡ã€‚

####  **State & Reward Design analysis**

| å˜é‡å                                | å«ä¹‰                  | å–å€¼èŒƒå›´ / ç±»å‹ | è®¡ç®—æ–¹å¼                                                     | ç”¨é€” / è¯´æ˜                                      |
| ------------------------------------- | --------------------- | --------------- | ------------------------------------------------------------ | ------------------------------------------------ |
| **State**                             |                       |                 |                                                              |                                                  |
| `query_clarity`                       | ç”¨æˆ·è¯·æ±‚æ¸…æ™°åº¦        | [0,1]           | ç”¨æˆ·è¯·æ±‚æ¸…æ™°åº¦ï¼Œã€åº”è¯¥èƒ½æ‰¾åˆ°å·²æœ‰çš„[bert-based/nlp-basedçš„æ–¹æ³•ï¼ŸğŸ” | è¡¨ç¤ºæ¨¡å‹å¯¹ç”¨æˆ·æ„å›¾æ¨¡ç³Šç¨‹åº¦çš„æ„ŸçŸ¥ï¼ˆæ˜¯å¦éœ€è¦æ¾„æ¸…ï¼‰ |
| `task_complexity`                     | ä»»åŠ¡å¤æ‚åº¦            | [0,1]           | ä»»åŠ¡å¤æ‚åº¦ï¼Œç”±ä»»åŠ¡é•¿åº¦ä¸å­ä»»åŠ¡æ•°ä¼°è®¡ï¼›ã€æœ€å¥½æ•°æ®é›†é‡Œé¢ç»™å‡ºäº† æ¯ä¸€æ¡çš„å¤æ‚åº¦ ğŸ” | è¡¨ç¤ºä»»åŠ¡çš„éš¾åº¦ä¸ä¿¡æ¯å¯†åº¦                         |
| `dialogue_turn`                       | å½“å‰è½®æ¬¡              | Nï¼ˆæ•´æ•°ï¼‰       | å¯¹è¯è®¡æ•°å™¨                                                   | æ§åˆ¶å¤šè½®äº¤äº’èŠ‚å¥                                 |
| `N_prev_questions`                    | ä¸Šä¸€è½®æ¾„æ¸…é—®é¢˜æ•°      | {0,1,2}         | ç›´æ¥è®¡æ•°                                                     | åæ˜ ä¸Šä¸€è½®ä¸»åŠ¨æ€§å¼ºåº¦                             |
| `N_prev_rejects`                      | ä¸Šä¸€è½®æ˜¾å¼/éšå¼æ‹’ç»æ•° | {0,1,2}         | æ˜¾å¼åˆ¤æ–­æ•° + éšå¼åˆ¤æ–­æ•°                                      | ç›´æ¥ç»™                                           |
| `h_ctx`                               | å¯¹è¯ä¸Šä¸‹æ–‡åµŒå…¥        | å‘é‡            | å°å‹ç¼–ç å™¨ï¼ˆRoBERTa / Llama LoRAï¼‰                           | æ•æ‰è¯­ä¹‰ä¸Šä¸‹æ–‡çš„å…¨å±€çŠ¶æ€                         |
| **Reward**                            |                       |                 |                                                              |                                                  |
| R_task                                | ä»»åŠ¡å®Œæˆåº¦å¾—åˆ†        | [0,1]           | æ ¹æ®ä»»åŠ¡é€‰æ‹©æŒ‡æ ‡                                             | ä»»åŠ¡å¯¼å‘çš„æ­£å‘å¥–åŠ±                               |
| C_interrupt                           | å¹²æ‰°æˆæœ¬              | [0,âˆ)           | C_interrupt = w1 * N_questions + w2 * N_user_rejects + w3 * N_long_turns + w4 * N_off_topic | æƒ©ç½šè¿‡åº¦æ‰“æ–­ã€è¢«æ‹’ã€å†—é•¿ã€è·‘é¢˜ç­‰è¡Œä¸º             |
| `N_questions`                         | å½“å‰è½®æ¾„æ¸…é—®é¢˜æ•°      | {0,1,2}         | ç›´æ¥è®¡æ•°                                                     | è¡¡é‡ä¸»åŠ¨å‘é—®çš„å¼ºåº¦                               |
| `N_user_reject`                       | å½“å‰è½®æ˜¾å¼æ‹’ç»æ•°      | {0,1,2}         | å…ˆåˆ¤æ–­æ˜¾ç¤ºæ‹’ç» ï¼ˆåŒ¹é…æ‹’ç»çŸ­è¯­ï¼šâ€œåˆ«é—®äº†ï¼Œç›´æ¥ç»™ï¼Œä¸ç”¨é—®ï¼Œä¸è¦è§£é‡Šï¼Œå¿«ç‚¹ï¼Œåˆ«åºŸè¯â€ï¼‰ | è¡¡é‡ç”¨æˆ·ä¸æ»¡ç¨‹åº¦                                 |
| `N_long_turns`                        | å½“å‰è½®è¾“å‡ºè¿‡é•¿        | {0,1}           | è‹¥è¾“å‡º token > Ï„ï¼ˆå¦‚ 200ï¼‰åˆ™ä¸º 1                             | æƒ©ç½šå†—é•¿å“åº”                                     |
| `N_off_topic`                         | å½“å‰è½®è·‘é¢˜æ•°          | {0,1}           | åˆ¤æ–­éšå¼æ‹’ï¼Œå¯ä»¥é€šè¿‡è¯­ä¹‰ç›¸ä¼¼åº¦ (sentence embedding)æˆ–è€…ç°æˆ NLP QA Matching æ–¹æ³• åˆ¤æ–­ ğŸ” | æƒ©ç½šåç¦»ä»»åŠ¡ä¸»é¢˜                                 |
| R_t = R_task - $\alpha$ * C_interrupt | ç»¼åˆå¥–åŠ±              |                 | Î± â‰ˆ 0.6ï¼ˆå¯è‡ªé€‚åº”è°ƒæ•´ï¼‰                                      | å¹³è¡¡ä»»åŠ¡æˆåŠŸä¸å¹²æ‰°æˆæœ¬                           |

**ä¼˜ç‚¹ï¼š**

1. State-Rewardé‡å æ˜¯åˆç†çš„ï¼ˆä¸Šä¸€è½® vs å½“å‰è½® Atariç±»ä»»åŠ¡ï¼šStateå’ŒRewardä¸é‡å ï¼Œå¯¹è¯ç±»ä»»åŠ¡ï¼šç»å¸¸é‡å ï¼‰
2. æ˜¾å¼ç‰¹å¾æ›´å¯è§£é‡Šï¼ˆçŸ¥é“agentåœ¨å…³æ³¨ä»€ä¹ˆï¼‰
3. Rewardè®¾è®¡æ›´ç²¾å‡†ï¼ˆæ˜ç¡®æƒ©ç½šæ‰“æ‰°æˆæœ¬ï¼‰
4. é€‚åˆä½ çš„ç ”ç©¶é—®é¢˜ï¼ˆcalibrating proactivityï¼‰

**ç¼ºç‚¹ï¼š**

1. Feature engineeringå·¥ä½œé‡å¤§ï¼ˆéœ€è¦å®šä¹‰å¦‚ä½•æå–ï¼‰
2. è°ƒå‚ç©ºé—´å¤§ï¼ˆw1-w4, Î±éƒ½éœ€è¦è°ƒï¼‰
3. å¯èƒ½è¿‡æ‹Ÿåˆï¼ˆç‰¹å¾è®¾è®¡å¤ªspecificï¼‰



å¦‚æœéœ€è¦è¿›ä¸€æ­¥ç®€åŒ–ç®—æ³•çš„è®¾è®¡ï¼š

æ–¹æ¡ˆ1:

Stateï¼ˆå»ºè®® 4 ç»´ï¼Œè€Œé 3 ç»´ï¼‰

- `query_clarity âˆˆ [0,1]`
- `task_complexity âˆˆ [0,1]`
- `dialogue_turn âˆˆ â„•`
- `N_prev_rejects âˆˆ {0,1,2}` â† å¼ºçƒˆå»ºè®®ä¿ç•™ï¼ˆç”¨æˆ·è€å—åº¦çš„æœ€è¿‘è¯æ®ï¼‰

> å¯é€‰ï¼š`h_ctx`ï¼ˆå°ç¼–ç å™¨ï¼‰åšå¤‡ä»½ï¼Œä¸è¡Œå°±å…³ã€‚

Rewardï¼ˆå»ºè®® 3 é¡¹ï¼Œè€Œé 2 é¡¹ï¼‰
$$
R = R_{\text{task}} - \alpha \cdot \big( 0.4\,N_{\text{questions}} + 0.9\,N_{\text{rejects}} + 0.2\,N_{\text{long\_turns}} \big)
$$

- `N_rejects` = æ˜¾å¼æ‹’ç» + éšå¼æœªå›åº”ï¼ˆæŠŠ `off_topic` å¹¶è¿›å»ï¼‰
- `N_long_turns` = (token > Ï„) âˆˆ {0,1}ï¼ˆå‡ ä¹é›¶æˆæœ¬ï¼Œèƒ½é˜²å•°å—¦ï¼‰
- `N_questions` capped â‰¤ 2ï¼ˆä¸åŠ¨ä½œç©ºé—´ä¸€è‡´ï¼‰

> è¿™ç‰ˆ**æˆæœ¬ä½**ã€**ä¿¡å·è¶³**ã€**è°ƒå‚å°‘**ï¼Œä¸”å’Œä½ çš„ç ”ç©¶é—®é¢˜ï¼ˆcalibrating proactivityï¼‰é«˜åº¦ä¸€è‡´ã€‚



æ–¹æ¡ˆ2:

ç®€åŒ–Stateï¼ˆ6ç»´ -> 3ç»´ï¼‰ï¼š

- query_clarity
- task_complexity
- dialogue_turn

Rewardï¼ˆ4ç»´ -> 2ç»´ï¼‰ï¼š 

- R = R_task - Î± Ã— (0.3 Ã— N_questions + 1.0 Ã— N_rejects)

å»æ‰ï¼š

- Stateé‡Œçš„N_prev_*ï¼ˆç”¨dialogue_turnä»£æ›¿ï¼‰
- Rewardé‡Œçš„N_long_turnså’ŒN_off_topic



#### 1.5 å­¦ä¹ èŒƒå¼ï¼ˆHow to trainï¼‰

- å˜ä½“Aï½œSFTï¼ˆç¦»çº¿ï¼‰

  - ç›®æ ‡ï¼šå…ˆå­¦åŸºæœ¬åŠ¨ä½œæ¨¡æ¿ä¸ç¨³å¥è¾“å‡ºã€‚

  - æ•°æ®ï¼šå¤šè½®ä»»åŠ¡å¯¹è¯ + æˆ‘ä»¬çš„ä¸‰æ¡£åŠ¨ä½œæ¨¡æ¿ï¼ˆLow/Mid/Highï¼‰ã€‚

  - ç›‘ç£ç›®æ ‡ï¼šè®©æ¨¡å‹èƒ½æ­£ç¡®è°ƒç”¨å¯¹åº”æ¨¡æ¿ï¼Œä¸è·‘åã€‚

- å˜ä½“Bï½œOffline DPOï¼ˆé¦–é€‰MVPï¼‰

  - ç›®æ ‡ï¼šå­¦ä¼š â€œåœ¨ç»™å®šåœºæ™¯ s_t ä¸‹é€‰å¯¹ä¸»åŠ¨æ€§å¼ºåº¦ a_tâ€ã€‚

  - åå¥½å¯¹æ„é€ ï¼šåŒä¸€ `s_t` ä¸‹ï¼Œå¯¹æ¯” `{Low, Mid, High}` çš„å“åº”ï¼ŒæŒ‰
     `score = R_task âˆ’ Î±Â·C_interrupt` é€‰ä¼˜/åŠ£ã€‚

  - è®­ç»ƒï¼šDPO/IPO ç›´æ¥ä¼˜åŒ– `Ï€(a|s)`ï¼ˆæ— éœ€æ˜¾å¼RMï¼‰ã€‚

- å˜ä½“Cï½œOnline PPOï¼ˆè¿›é˜¶ï¼Œå¯é€‰ï¼‰

  - ç›®æ ‡ï¼šåœ¨çº¿æ¢ç´¢ï¼Œç»†è°ƒç­–ç•¥è¾¹ç•Œä¸é˜ˆå€¼ã€‚
  - ç¯å¢ƒï¼šç”¨æˆ·æ¨¡æ‹Ÿå™¨ï¼ˆå«ä¸åŒç”¨æˆ·åå¥½åŸå‹ï¼‰ã€‚

  - å¥–åŠ±ï¼š`R_task âˆ’ Î±Â·C_interrupt`ï¼ˆåŠ å…¥åŠ¨ä½œå®‰å…¨çº¦æŸï¼šæ‹’ç»è¯â†’å¼ºåˆ¶ Lowï¼›åŒè½®æœ€å¤š2é—®ï¼‰ã€‚
  - ç”¨é€”ï¼šæŠŠ Offline DPO å­¦åˆ°çš„ç­–ç•¥å†â€œæ‰“ç£¨â€ã€‚

- å˜ä½“Dï½œOnline DPOï¼ˆè½»é‡åœ¨çº¿ï¼‰

  - ç›®æ ‡ï¼šä½æˆæœ¬åœ¨çº¿æ ¡å‡†åå¥½ã€‚

  - åšæ³•ï¼šä¸Šçº¿å°æµé‡æ”¶é›†æ–°åå¥½å¯¹ï¼ˆçœŸå®æ‹’ç»/å¿½ç•¥ï¼‰ï¼Œå‘¨æœŸæ€§å¢é‡ DPOã€‚

  - ä½œç”¨ï¼šåŠ¨æ€è´´åˆçœŸå®ç”¨æˆ·çš„â€œæ‰“æ‰°é˜ˆå€¼â€ã€‚



#### 1.6 è¯„ä¼°ï¼ˆå…ˆå®šâ€œå¿…æŠ¥â€ä¸¤ç±»ï¼‰

- **ä»»åŠ¡æˆåŠŸ**ï¼špass@k / tests / LLM Judgeã€‚
- **æ‰“æ‰°æˆæœ¬**ï¼š`C_interrupt` åŠå„é¡¹åˆ†é‡ç»Ÿè®¡ã€‚
- **å¯è§†åŒ–**ï¼š**Pareto æ›²çº¿ï¼ˆæˆåŠŸç‡â€“æˆæœ¬ï¼‰**ï¼Œå±•ç¤ºâ€œå­¦ä¼šåˆ†å¯¸â€ã€‚



# 10.22

æˆ‘è¿™ä¸ªideaæ˜¯è¯»äº†collabllmä¹‹åäº§ç”Ÿçš„ï¼Œæ‰€ä»¥å®ƒå’Œcollabllmå…¶å®æœ‰ç‚¹åƒ

collabllmé‚£ç¯‡æ–‡ç« æˆ‘è§‰å¾—ä¸»è¦çš„åˆ›æ–°æ˜¯proactiveè¿™ä¸ªæ¦‚å¿µå§ï¼ˆå®ƒçš„æ•…äº‹è®²çš„ç‰¹åˆ«å¥½ï¼‰ï¼Œ1. å®ƒå¼ºè°ƒäº†multiturnè¿™ä¸ªä¸œè¥¿ï¼Œ2.ç„¶åå°è¯•è®©llm act like a real human collaborator 3. ç„¶åå®ƒç”¨äº†ä¸‰ä¸ªä¸åŒçš„æ•°æ®é›† ä½“ç°äº†å®ƒè¿™ä¸ªideaçš„æ³›ç”¨æ€§

æˆ‘è¿™ç¯‡æ–‡ç« è¦æ€ä¹ˆåšçš„æ›´åŠ innovativeï¼Œè€Œä¸ä»…ä»…åƒæ˜¯collallmçš„incrementalå‘¢? 

1. æˆ‘æœ¬æ¥æƒ³è¦é€‰ç”¨å’Œcollabllmä¸€æ ·çš„æ•°æ®é›†çš„ï¼Œå› ä¸ºå®ƒå·²ç»è¯æ˜å®ƒå¯è¡Œäº†ï¼Œä½†æ˜¯ç°åœ¨ä¸€æƒ³ï¼Œå¦‚æœç”¨ä¸€æ ·çš„ï¼Œé‚£å¤ªå®¹æ˜“è¢«è´¨ç–‘incrementaläº†

2. æˆ‘è¦æ€ä¹ˆè®²æˆ‘çš„è¿™ä¸ªstoryå‘¢ï¼Ÿé€‰ä»€ä¹ˆæ ·çš„taskæ¯”è¾ƒå¥½ï¼Ÿä»€ä¹ˆæ ·çš„taskå¯¹äºè¿™ä¸ªproactive levelç‰¹åˆ«æ•æ„Ÿå‘¢ï¼Ÿä½ å¸®æˆ‘æ€è€ƒä¸€ä¸‹é€‰æ‹©ä»€ä¹ˆæ ·çš„ä»»åŠ¡èƒ½è®©æˆ‘è¿™ä¸ªRLæ—¢è¡¨ç°çš„å¥½ åˆèƒ½è®©æˆ‘è¿™ä¸ªstoryè®²çš„å¥½

   

## è®ºæ–‡æ•…äº‹ç»“æ„

**èƒŒæ™¯**ï¼šä¸»åŠ¨æ€§ï¼ˆproactivityï¼‰æå‡ LLM åä½œæ•ˆæœï¼Œä½†ä¹Ÿå¸¦æ¥è¿‡åº¦å¹²é¢„çš„é—®é¢˜ï¼ˆå¹²æ‰°ã€å†—é•¿ã€è¢«æ‹’ï¼‰ã€‚

**åŠ¨æœº**ï¼šç°æœ‰æ–¹æ³•ï¼ˆå¦‚ CollabLLMï¼‰ é€šå¸¸ä»¥å›ºå®šæˆ–å¹³å‡æ°´å¹³çš„ä¸»åŠ¨æ€§è¿›è¡Œä¼˜åŒ–ï¼Œç¼ºä¹æ ¹æ®ä¸Šä¸‹æ–‡å·®å¼‚ï¼ˆcontextual cuesï¼‰åŠ¨æ€è°ƒèŠ‚ä¸»åŠ¨æ€§çš„èƒ½åŠ›ã€‚æ¨¡å‹æ— æ³•ç†è§£â€œåˆ¤æ–­ä½•æ—¶è¯¥ä¸»åŠ¨ã€ä¸»åŠ¨åˆ°ä»€ä¹ˆç¨‹åº¦â€ã€‚

**ç ”ç©¶ç›®æ ‡**ï¼šæˆ‘ä»¬å¸Œæœ›è®©æ¨¡å‹å­¦ä¼šcontext-aware proactivityï¼šèƒ½å¤Ÿæ„ŸçŸ¥å½“å‰ä»»åŠ¡ä¸å¯¹è¯çŠ¶æ€ï¼Œåˆ¤æ–­ä½•æ—¶åº”æå‡ºæ¾„æ¸…æˆ–å»ºè®®ã€ä¸»åŠ¨æ€§åº”è¾¾åˆ°ä½•ç§ç¨‹åº¦ï¼Œä»è€Œå®ç°æ›´è‡ªç„¶ã€æ›´é«˜æ•ˆçš„äººæœºåä½œã€‚

**æ–¹æ³•**ï¼šæå‡º RL æ¡†æ¶ï¼Œåˆ©ç”¨ state è¡¨å¾åœºæ™¯ç‰¹å¾ï¼ˆclarity, complexity, turn, user rejectionï¼‰ï¼Œreward æ˜¾å¼å¹³è¡¡æˆåŠŸç‡ä¸å¹²æ‰°æˆæœ¬ï¼Œä½¿æ¨¡å‹åœ¨ä¸åŒæƒ…å¢ƒä¸‹å­¦ä¹ æœ€ä¼˜ä¸»åŠ¨æ€§ç­–ç•¥ã€‚

**è´¡çŒ®**ï¼š

- æå‡ºâ€œ**context-aware proactivity**â€ æ¦‚å¿µï¼Œå¼ºè°ƒä¸»åŠ¨æ€§åº”ä¾èµ–äºä¸Šä¸‹æ–‡è€Œéå›ºå®šæ°´å¹³ï¼›
- è®¾è®¡ä¸€ä¸ªå¯æ³›åŒ–çš„ RL æ¡†æ¶ä»¥å­¦ä¹ åŸºäºä¸Šä¸‹æ–‡çš„ä¸»åŠ¨æ€§è°ƒèŠ‚ï¼›
- åœ¨å¯¹è¯ä»»åŠ¡ä¸­å±•ç¤ºæ¨¡å‹èƒ½è‡ªé€‚åº”åœ°è°ƒæ•´ä¸»åŠ¨è¡Œä¸ºï¼Œæå‡ä»»åŠ¡æˆåŠŸç‡åŒæ—¶å‡å°‘ä¸å¿…è¦çš„å¹²æ‰°ã€‚

ä¸€å¥ summary å¯ä»¥è¿™æ ·å†™ï¼š

> We move beyond making LLMs merely proactive toward making them **context-aware collaborators**â€”agents that learn *when* and *how much* to engage based on the ongoing dialogue and task context.



ä»»åŠ¡çš„é€‰æ‹©

å…³é”®æ€è·¯ï¼šé€‰é‚£äº› **â€œä¸»åŠ¨è¡Œä¸ºæ—¢èƒ½å¸®åŠ©ä¹Ÿå¯èƒ½æƒ¹æ¼ç”¨æˆ·â€** çš„ä»»åŠ¡ã€‚

| ä»»åŠ¡ç±»å‹                                   | ä¸ºä»€ä¹ˆæ•æ„Ÿ                               | å…¸å‹åœºæ™¯                                   | å¯æµ‹æŒ‡æ ‡                        |
| ------------------------------------------ | ---------------------------------------- | ------------------------------------------ | ------------------------------- |
| **Code Generation / Debugging** ğŸ§‘â€ğŸ’»         | ä¸»åŠ¨é—®å¤ªå¤šæ‰“æ–­ï¼Œå¤ªå°‘åˆ™ç†è§£é”™è¯¯           | ç”¨æˆ·è¦ä¸€ä¸ªå‡½æ•°ï¼ŒAIå¯é—®ï¼šç”¨å“ªåº“ï¼Ÿè¾“å‡ºæ ¼å¼ï¼Ÿ | Pass@k, rejectç‡, è½®æ¬¡          |
| **Creative Writing / Story Co-Creation** âœï¸ | ä¸»åŠ¨å»ºè®®å¤ªå¤š = æŠ¢é£å¤´ï¼›å¤ªå°‘ = æ— å¼•å¯¼     | AIé—®â€œä¸»è§’æ˜¯è°â€ vs ç›´æ¥å†™                   | LLM-Judgeå¾—åˆ†, ç”¨æˆ·æ¥å—åº¦       |
| **Task Planning / Todo Assistant** ğŸ“‹       | ç”¨æˆ·ç›®æ ‡å¸¸æ¨¡ç³Šï¼ŒAIéœ€é—®ä¼˜å…ˆçº§ï¼Œä½†ä¸èƒ½å•°å—¦ | â€œå¸®æˆ‘æ•´ç†ä»Šå¤©å·¥ä½œâ€                         | Plan accuracy, è½®æ¬¡, ç”¨æˆ·ç¡®è®¤ç‡ |
| **Data Query or Table QA** ğŸ“Š               | æ¨¡ç³Š query æ—¶æ¾„æ¸…èƒ½ææ•ˆï¼Œä½†è¿‡å¤šæµªè´¹æ—¶é—´  | â€œç»™æˆ‘ç»Ÿè®¡é”€å”®é¢â€                           | å‡†ç¡®ç‡, å¯¹è¯è½®æ¬¡                |
| **Knowledge-Grounded QA / Teaching** ğŸ“     | ä¸åŒç”¨æˆ·å®¹å¿åº¦å·®å¼‚å¤§                     | AIé—®â€œæ˜¯å¦æƒ³è¦è¯¦ç»†è§£é‡Šï¼Ÿâ€                   | F1, ç”¨æˆ·æ»¡æ„åº¦                  |

collabllmè¿™ç¯‡æ–‡ç« ï¼Œå®ƒé€‰äº†3ä¸ªä»»åŠ¡ï¼Œ1ä¸ªbaseæ¨¡å‹ (Llama-3.1-8B-Instruct) ï¼Œ4ç§å˜ä½“SFTï¼ŒPPOï¼ŒOffline DPOï¼ŒOnline DPO

æˆ‘çš„è¿™ç¯‡æ–‡ç« ä½ è§‰å¾—æ€ä¹ˆåšå¥½å‘¢ï¼Ÿ2ä¸ªä»»åŠ¡ï¼ˆcode generation/debuggingå’Œtask planning/todo assitantï¼‰ï¼Œ Nä¸ªbaseï¼ˆå‡ ä¸ªåˆé€‚ï¼Ÿï¼‰æ¨¡å‹ï¼ŒNä¸ªå˜ä½“ï¼ˆå‡ ä¸ªåˆé€‚ï¼Ÿï¼ŒPPOè¿˜æ˜¯DPOï¼Œ onlineè¿˜æ˜¯offlineå‘¢ ä½ æœ‰ä»€ä¹ˆå»ºè®®å— è€ƒè™‘åˆ°æˆ‘çš„èµ„æºæœ‰é™åˆ¶ï¼‰



å®éªŒç»“æ„

| ç±»åˆ«                 | æ•°é‡            | å†…å®¹                                                   | ç›®çš„                              |
| -------------------- | --------------- | ------------------------------------------------------ | --------------------------------- |
| **ä»»åŠ¡**             | 2               | Code Generation / Task Planning                        | ä½“ç°ä¸»åŠ¨æ€§å¯¹ä¸åŒ context çš„é€‚åº”æ€§ |
| **Base æ¨¡å‹**        | 1 (+1 optional) | Llama-3.1-8B-Instruct (+ Mistral-7B)                   | å¯å¯¹æ¯” CollabLLMï¼Œç»“æœæœ‰å¤–æ¨æ€§    |
| **å˜ä½“**             | 3               | (1) SFT baseline (2) Rule-based (3) Offline DPO (Ours) | å±•ç¤ºç®—æ³•å¸¦æ¥çš„æ˜¾è‘—æå‡            |
| **å¯é€‰é™„åŠ **         | +1              | PPO (å°è§„æ¨¡)                                           | éªŒè¯æ–¹æ³•ä¸€è‡´æ€§                    |
| **è¯„ä¼°æŒ‡æ ‡ï¼ˆæš‚å®šï¼‰** | 5               | Task Success, Interaction Cost, PSA, QE, RTA           | æˆåŠŸç‡ + åˆ†å¯¸æ„ŸåŒç›®æ ‡è¯„ä¼°         |



| ç±»åˆ«                           | æ•°é‡             | å†…å®¹                                                         | ç›®çš„                                                         |
| ------------------------------ | ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| ä»»åŠ¡ï¼ˆTasksï¼‰                  | 2                | 1. Code Generation / Debuggingï¼ˆå®¢è§‚ä»»åŠ¡ï¼‰ <br />2. Task Planning / To-Do Assistantï¼ˆä¸»è§‚ä»»åŠ¡ï¼‰ | ä½“ç°ä¸»åŠ¨æ€§å¯¹ä¸åŒ context typesï¼ˆæŠ€æœ¯æ€§ vs è®¤çŸ¥æ€§ï¼‰ çš„é€‚åº”èƒ½åŠ› |
| Base æ¨¡å‹ï¼ˆBase LLMsï¼‰         | 1 (+ 1 optional) | Llama-3.1-8B-Instructï¼ˆä¸»æ¨¡å‹ï¼‰<br /> Mistral-7B-Instructï¼ˆoptionalï¼‰ | ä¿æŒä¸ CollabLLM ä¸€è‡´ä¾¿äºå¯¹æ¯”ï¼ŒåŒæ—¶éªŒè¯æ–¹æ³•çš„å¤–æ¨æ€§          |
| è®­ç»ƒå˜ä½“ï¼ˆTraining Variantsï¼‰  | 3                | (1) **SFT baseline** â€“ çº¯ç›‘ç£æ¨¡å‹ï¼Œæ— ä¸»åŠ¨æ§åˆ¶ <br />(2) **Offline DPO (Ours)** â€“ åŸºäºä¸Šä¸‹æ–‡çš„ä¸»åŠ¨æ€§è‡ªæ ¡å‡†ç­–ç•¥ <br />(3) **PPO (small-scale) ** (è´µ å›°éš¾ æš‚å®šï¼‰ | å±•ç¤ºä»æ— ä¸»åŠ¨æ€§ â†’ å¯å‘å¼ â†’ å­¦ä¹ å‹ çš„æ€§èƒ½æå‡æ¢¯åº¦              |
| è¯„ä¼°æŒ‡æ ‡ï¼ˆMetrics, tentativeï¼‰ | 5                | Task Success Rateï¼ˆä»£ç æ­£ç¡®ç‡ / ä»»åŠ¡å®Œæˆåº¦ï¼‰ Interaction Cost (IC) â€“ æ‰“æ‰°æˆæœ¬ Proactivity Sensitivity Analysis (PSA) â€“ æ¨¡å‹å¯¹æƒ…å¢ƒæ•æ„Ÿåº¦ Query Efficiency (QE) â€“ å¹³å‡å®Œæˆå›åˆæ•° Response Tone Alignment (RTA) â€“ è¯­æ°”ä¸ç”¨æˆ·åå¥½åŒ¹é…åº¦ | åŒæ—¶è¡¡é‡æ¨¡å‹çš„ å®Œæˆèƒ½åŠ›ä¸åˆ†å¯¸æ„Ÿï¼ˆappropriateness of proactivityï¼‰ |



æˆ‘æœ‰ä¸ªé—®é¢˜ æˆ‘è¿™ä¸ªé¡¹ç›®é‡Œé¢è¦ä½¿ç”¨çš„è½¨è¿¹ å¯ä»¥ç”¨åˆ«çš„æ–‡ç« é‡Œé¢ç”Ÿæˆå¥½çš„å—

RLé¡¹ç›®äº†çš„è§„åˆ™ä¸€èˆ¬æ˜¯æ€ä¹ˆç”Ÿæˆçš„å‘¢ï¼Ÿæ ¹æ®ä»»åŠ¡è®¾è®¡ä¸€å¥—promptså§

æˆ‘çš„concernæ˜¯ æˆ‘è¿™ä¸ªé¡¹ç›®æ˜¯å¸Œæœ›llmèƒ½å¤Ÿæ™ºèƒ½çš„æ§åˆ¶ä¸»åŠ¨çš„å¼ºåº¦ é‚£æˆ‘éœ€è¦çš„è½¨è¿¹æ˜¯è¦æœ‰ä¸€å®šåŒºåˆ†åº¦çš„å§ï¼ˆä¸åŒä»»åŠ¡é‡Œ ä¸åŒéš¾åº¦ ä¸åŒåå¥½ï¼‰å…¶ä»–æ–‡ç« é‡Œå·²ç»ç”Ÿæˆçš„è½¨è¿¹å¯èƒ½ä¸ç¬¦åˆå§ï¼Ÿï¼Ÿ 

æˆ‘è¦è‡ªå·±åšä¸ªuser simulatorå‘¢ï¼Ÿè¿˜æ˜¯æ‰¾æ‰¾æœ‰æ²¡æœ‰ä»€ä¹ˆç°æˆèƒ½ç”¨çš„æ•°æ®ï¼Ÿ



# 10.23

## æ€è€ƒä¸‹æˆ‘è¿™ç¯‡æ–‡ç« çš„åˆ›æ–°ç‚¹

ä¸ CollabLLM çš„å‰ç»æ€§å¤šè½®å¥–åŠ±ä¸åŒï¼Œæœ¬ç ”ç©¶å…³æ³¨**ä¸»åŠ¨æ°´å¹³çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§**ï¼Œå³æ¨¡å‹æ˜¯å¦èƒ½æ ¹æ®è¿‡å¾€å¯¹è¯å†å²ã€ä»»åŠ¡å¤æ‚åº¦ä¸ç”¨æˆ·ååº”ï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´ä¸»åŠ¨ç¨‹åº¦ã€‚æˆ‘ä»¬ä¸æ˜¯é¢„æµ‹æœªæ¥å¥–åŠ±åˆ†å¸ƒï¼Œè€Œæ˜¯æ˜¾å¼å»ºæ¨¡è¿‡å»è¡Œä¸ºè½¨è¿¹å¯¹å½“å‰å†³ç­–çš„å½±å“ï¼Œä½¿å¾— agent çš„ä¸»åŠ¨æ€§åœ¨æ—¶é—´ç»´åº¦ä¸Šæ›´åŠ ç¨³å®šã€å¯æ§ã€ç¬¦åˆè¯­å¢ƒã€‚



CollabLLM çš„ **multi-turn reward** æ˜¯ **å‰ç»å¼ï¼ˆforward-lookingï¼‰** çš„ï¼š

> å®ƒå‡è®¾â€œä¸€ä¸ªå¥½çš„å½“å‰è¡ŒåŠ¨ = èƒ½å¸¦æ¥æ›´é«˜æœªæ¥æ€»ä½“æ”¶ç›Šâ€ã€‚
>  å³ï¼Œreward depends on *how this turn influences future turns.*

è€Œä½ çš„è®¾è®¡æ˜¯ **å›é¡¾å¼ï¼ˆbackward/context-awareï¼‰** çš„ï¼š

> ä½ å‡è®¾â€œä¸€ä¸ªè‰¯å¥½çš„å½“å‰è¡ŒåŠ¨åº”å½“æ ¹æ®**æ—¢æœ‰çš„å¯¹è¯ä¸Šä¸‹æ–‡**è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ï¼ŒåŒ…æ‹¬å†å²ä¸»åŠ¨æ€§è½¨è¿¹ã€ç”¨æˆ·è¯­æ°”ä¸ä»»åŠ¡å¤æ‚åº¦ç­‰å› ç´ ã€‚â€ã€‚
>  å³ï¼Œthe reward depends on how appropriately the current action aligns with the established interactional pattern.



| æ¯”è¾ƒç»´åº¦ | CollabLLM                            | ä½ çš„æ¡†æ¶ï¼ˆContext-Aware RLï¼‰                              |
| -------- | ------------------------------------ | --------------------------------------------------------- |
| å¥–åŠ±ä¾èµ– | æœªæ¥è½¨è¿¹ $t_{j+1:K}$                 | è¿‡å»è½¨è¿¹ $t_{1:j-1}$                                      |
| å¥–åŠ±æ ¸å¿ƒ | â€œé¢„æµ‹æœªæ¥æ”¶ç›Šâ€                       | â€œæ ¡å‡†å½“å‰è¡Œä¸ºä¸ä¸Šä¸‹æ–‡åŒ¹é…åº¦â€                              |
| ä¼˜åŒ–ç›®æ ‡ | å¢å¼ºé•¿è¿œä»»åŠ¡ç»©æ•ˆï¼ˆgoal achievementï¼‰ | å¢å¼ºå½“ä¸‹äº’åŠ¨åˆç†æ€§ä¸è¿è´¯æ€§ï¼ˆinteraction appropriatenessï¼‰ |
| æ—¶é—´æ–¹å‘ | å‰ç»å¼ï¼ˆforward-lookingï¼‰            | å›é¡¾å¼ï¼ˆbackward-sensitiveï¼‰                              |
| å½±å“é€»è¾‘ | this turn â†’ future outcome           | past context â†’ this turnâ€™s appropriateness                |



**State çš„è®¾è®¡ç¡®å®å¯ä»¥æ‰©å±•æˆã€Œè¿‡å»å¤šè½®ã€çš„ä¾èµ–ï¼š**

ä½ ç°åœ¨çš„ state åªåŒ…å«ä¸Šä¸€è½®ï¼ˆæˆ–éƒ¨åˆ†ä¸Šä¸‹æ–‡ï¼‰ â†’ è¿™é™åˆ¶äº†å®ƒå¯¹â€œé•¿æœŸè¯­å¢ƒæ¨¡å¼â€çš„ç†è§£ã€‚

âœ… å»ºè®®æ”¹è¿›æ€è·¯ï¼š

1. **å¼•å…¥å†å²çª—å£ (Context Window)**ï¼š
    ä¾‹å¦‚ï¼Œstate åŒ…å«æœ€è¿‘ N è½®ï¼ˆN=3~5ï¼‰çš„ï¼š
   - ç”¨æˆ·è¯­æ°” / æ˜ç¤ºéœ€æ±‚
   - æ¨¡å‹ä¸»åŠ¨ç¨‹åº¦ (question frequency, initiative ratio)
   - ç”¨æˆ·å“åº”å»¶è¿Ÿ / æ¥å—åº¦
2. **æå–ç»Ÿè®¡ç‰¹å¾è€ŒéåŸå§‹æ–‡æœ¬**ï¼š
    ä¾‹å¦‚ï¼š
   - å¹³å‡ä¸»åŠ¨æ€§ï¼ˆAvg_Proact_Levelï¼‰
   - ä¸»åŠ¨å˜åŒ–è¶‹åŠ¿ï¼ˆÎ”Proactï¼‰
   - å†å²ä¸€è‡´æ€§å¾—åˆ†ï¼ˆConsistency w.r.t. contextï¼‰
3. **ç”¨ state encoderï¼ˆLSTM æˆ– attention poolingï¼‰ç¼–ç å¤šè½®ä¸Šä¸‹æ–‡**ï¼Œè€Œéç®€å•æ‹¼æ¥æ–‡æœ¬ã€‚

è¿™æ ·ï¼Œä½ çš„ state å…¶å®æ˜¯â€œcontext-aware memory representationâ€ï¼Œå®ƒåˆ»ç”»è¿‡å»è¯­å¢ƒå¯¹å½“å‰è¡ŒåŠ¨çš„éšæ€§çº¦æŸã€‚





## user simulator è½¨è¿¹çš„ç”Ÿæˆ

CollabLLM - forward sampleè½¨è¿¹

ä»–ä»¬çš„ MR å®šä¹‰æ˜¯ï¼šå½“å‰è¡Œä¸ºçš„ reward = å®ƒåœ¨æ‰€æœ‰åç»­ turns çš„æœªæ¥è¡¨ç°çš„æœŸæœ›

å› æ­¤ä»–ä»¬å¿…é¡»ï¼š

- roll outæœªæ¥å¤šè½®å¯¹è¯**
- ç”¨ user simulator + model ç»§ç»­å¯¹è¯
- å¾—åˆ°å®Œæ•´å¯¹è¯è½¨è¿¹
- å¯¹æ•´ä¸ªè½¨è¿¹æ‰“åˆ†



æˆ‘çš„ideaä¼˜åŒ–çš„æ˜¯ proactive levelï¼š

- â€œå½“å‰è¯¥ä¸è¯¥ä¸»åŠ¨â€
-  â€œå½“å‰ä¸»åŠ¨ç¨‹åº¦ä¸æœ€è¿‘ä¸Šä¸‹æ–‡æ˜¯å¦åŒ¹é…â€
-  â€œå‡å°‘è¿‡åº¦æé—®/æ‰“æ‰°â€

è¿™æ˜¯local interaction optimalityï¼Œä¸æ˜¯ long-horizon future task optimality

æ‰€ä»¥ä½ çš„ trajectory requirement æ˜¯ï¼Œç”Ÿæˆâ€œåŒä¸€ state ä¸‹ä¸åŒä¸»åŠ¨æ°´å¹³çš„åˆ†æ”¯è¾“å‡ºâ€ï¼š

- ç»™æ¯æ¡åˆ†æ”¯æ‰“ä¸€ä¸ªâ€œå½“ä¸‹ appropriatenessâ€åˆ†æ•°
- ç”¨ DPO å­¦ä¹ ç­–ç•¥

æˆ‘ä¸éœ€è¦æ»šæœªæ¥å¤šè½®å¯¹è¯ï¼Œä¸ä¾èµ–æœªæ¥ returnï¼Œåªéœ€ local turn + short past window



æˆ‘çš„ideaæ˜¯å¦æ°å½“ï¼š

CollabLLMï¼šcurrent action â†’ æ‰€æœ‰æœªæ¥ turns

ä½ ï¼špast few turns â†’ current action

ä½ æ‹…å¿ƒï¼šâ€œåˆ«äººæ˜¯ long-range horizonï¼Œæˆ‘åƒ sliding windowï¼Œä¼šä¸ä¼šå¼±/ä¸å¯è¡Œï¼Ÿâ€

ç­”æ¡ˆéå¸¸æ˜ç¡®ï¼š

> âœ… **ä½ çš„ idea å®Œå…¨å¯è¡Œï¼Œè€Œä¸”è¿™æ˜¯åˆç†çš„è®¾è®¡é€‰æ‹©ï¼Œä¸æ˜¯ç¼ºé™·ã€‚**

ç”šè‡³å¯ä»¥è¯´ï¼š

> **æœ‰é™çª—å£ï¼ˆtruncated dependencyï¼‰æ¯”æ— é™å›æº¯æ›´ç°å®ã€å¯æ§ã€å¯è®­ç»ƒï¼Œä¹Ÿæ˜¯ RL ä¸­éå¸¸å…¸å‹çš„åšæ³•ï¼ˆpartial observability / truncated historyï¼‰ã€‚**

ä¸‹é¢æˆ‘è§£é‡Šä¸ºä»€ä¹ˆã€‚

âœ… ä¸ºä»€ä¹ˆä½ â€œåªçœ‹æœ€è¿‘å‡ è½®â€æ˜¯åˆç†çš„ï¼Ÿ

1) äººç±»å¯¹è¯æœ¬èº«å°±æ˜¯å±€éƒ¨ Markov æ€§

å¯¹è¯çŠ¶æ€è®°å¿†æœ¬æ¥å°±è¡°å‡ã€‚äººç±»åˆ¤æ–­â€œè¯¥ä¸è¯¥é—®â€ä¸»è¦çœ‹çŸ­æœŸä¸Šä¸‹æ–‡ï¼š

> æœ€è¿‘å‡ å¥ç”¨æˆ·æ€åº¦ã€ä»»åŠ¡é˜¶æ®µã€æ˜¯å¦è¢«æ‹’ç»ã€æ¸…æ™°åº¦

ä¸ä¼šçœŸçš„è¿½è¸ª â€œç¬¬ 8 è½®æ—¶ç”¨æˆ·è¯´ xxx æˆ‘ç°åœ¨è¦è®°ä½â€ã€‚

è¿™ä¸ªç†å¿µåœ¨ HCI å’Œå¯¹è¯ç³»ç»Ÿå«åšï¼š

- local interaction state
- short-horizon pragmatic alignment
- situated interaction

è€Œä¸æ˜¯æ— é™ context.



# 10.30

## è½¨è¿¹ç”Ÿæˆ

### 1) å›ºå®šå¯¹è¯çŠ¶æ€ $s$

å–ä¸€ä¸ªçœŸå®æˆ–æ¨¡æ‹Ÿçš„å¤šè½®ä¸Šä¸‹æ–‡ï¼Œæ¯”å¦‚ï¼š

```python3
# User: my code has a bug
# Model turn j context: previous turns here...
```

å½¢æˆçŠ¶æ€: $s_t = (t_{1:t-1}, u_t)$



### 2) å¯¹åŒä¸€ä¸ª state ç”Ÿæˆä¸åŒ proactive responses

```python
# a_low  = minimal answer, no clarifying question
# a_mid  = short answer + 1 clarification
# a_high = detailed + multiple clarifications
```

è¿™äº›æ˜¯**ä¸‰ä¸ª candidate behaviors** = ä¸‰æ¡ mini-trajectory branches



### 3) è®¡ç®—æ¯ä¸ª $a_i$ çš„ reward



ä½¿ç”¨å›é¡¾å¼å³æ—¶ä¿¡å·ï¼š
$$
R_t = R_{\text{task}}(t) - \alpha \, C_{\text{interrupt}}(t)
$$

- **$R_{\text{task}}$**ï¼šä»»åŠ¡è¡¨ç°ï¼ˆæµ‹è¯•é€šè¿‡ç‡ / LLM Judge å¾—åˆ†ï¼‰
- **$C_{\text{interrupt}}$**ï¼šå¤šç»´å¹²æ‰°æˆæœ¬

$C_{\text{interrupt}}$ç”±å¤šä¸ª feature ç»„æˆï¼š

| reward component | è§£é‡Š             | å¯¹åº”å…¬å¼ç¬¦å·               |
| ---------------- | ---------------- | -------------------------- |
| ask cost         | æé—®æ¬¡æ•°         | $N_{\text{questions}}$     |
| user irritation  | ç”¨æˆ·æ‹’ç»ã€ä¸è€çƒ¦ | $N_{\text{user\_rejects}}$ |
| verbosity        | å•°å—¦ / é•¿å›å¤    | $N_{\text{long\_turns}}$   |
| off-topic        | èµ°å             | $N_{\text{off\_topic}}$    |
| coherence        | ä¸Šä¸‹æ–‡ä¸€è‡´æ€§     | å¯åšçŸ­çª—å£ penalty æˆ–ç‰¹å¾  |

$C_{\text{interrupt}} = w_1 N_{\text{questions}} + w_2 N_{\text{user\_rejects}} + w_3 N_{\text{long\_turns}} + w_4 N_{\text{off\_topic}}$



### 4) åå¥½æ’åº â†’ DPO è®­ç»ƒ

æ¯”è¾ƒå„æ¡£ä½å¾—åˆ†ï¼š

```python
# mid > low > high
# â†’ policy learns: in this context, mid proactive is best
```

ç”Ÿæˆåå¥½å¯¹ $(s_t, a^+, a^-)$ï¼Œ
ç”¨ DPO æˆ– preference learning ä¼˜åŒ–ç­–ç•¥ $\pi_{\text{gate}}(a|s_t)$ã€‚



## æ„å»ºuser simulator

**Persona å‚æ•°**

- `expertise âˆˆ {novice, intermediate, expert}`
- `patience âˆˆ [0,1]`ï¼ˆå®¹å¿è¢«é—®é—®é¢˜çš„ç¨‹åº¦ï¼‰
- `clarity âˆˆ [0,1]`ï¼ˆéœ€æ±‚æ˜¯å¦æ¸…æ™°ï¼‰
- `time_pressure âˆˆ {low, high}`
- `style âˆˆ {concise, polite, direct, verbose}`
- `domain âˆˆ {coding, planning}`
   -ï¼ˆå¯é€‰ï¼‰`risk_tolerance âˆˆ [0,1]`ï¼ˆè§„åˆ’é‡Œæ˜¯å¦æ¥å—ç²—ç•¥æ–¹æ¡ˆï¼‰

1. **ç»“æ„åŒ–ä¿¡å·ï¼ˆç»™ reward ç”¨ï¼‰**

- `answered_clarification âˆˆ {0,1}`
- `reject_signal âˆˆ {0,1}`ï¼ˆå¦‚â€œåˆ«é—®äº†ï¼Œç›´æ¥ç»™æ–¹æ¡ˆ/ä»£ç â€ï¼‰
- `silence âˆˆ {0,1}` æˆ–æçŸ­ç­”
- `off_topic_flag âˆˆ {0,1}`
- `satisfaction âˆˆ [0,1]`
   -ï¼ˆè®°å½•ï¼‰`user_goal_state`ï¼ˆè§„åˆ’ä»»åŠ¡è¿›åº¦ï¼‰/ `tests_passed`ï¼ˆç¼–ç é€šè¿‡æ•°ï¼‰

1. **è¡Œä¸ºè§„åˆ™ï¼ˆç®€å•ä½†å¯æ§ï¼‰**

- é‡‡çº³æ¾„æ¸…æ¦‚ç‡ï¼š`P(answer)= clarity * patience * g(expertise, time_pressure)`
- è§¦å‘æ‹’ç»ï¼šè¿ç»­æ¾„æ¸…â‰¥2 æˆ– `a=high & patience<0.4` â†’ `reject_signal=1`
- é•¿æ–‡æƒ©ç½šï¼šè‹¥åŠ©æ‰‹è¾“å‡º token>Ï„ ä¸” `patience<0.3` â†’ `silence=1`
- è·‘é¢˜åˆ¤å®šï¼šè¯­ä¹‰ç›¸ä¼¼åº¦ < Ï„ â†’ `off_topic_flag=1`
- æ»¡æ„åº¦ï¼šä¸â€œæ˜¯å¦å‰è¿›/æ˜¯å¦è§£å†³â€æ­£ç›¸å…³ï¼Œä¸â€œæ‹’ç»/è·‘é¢˜/å†—é•¿â€è´Ÿç›¸å…³

### A. Codingï¼ˆä¿® bug / è¡¥åŠŸèƒ½ï¼‰

- **ä»»åŠ¡ç›®æ ‡**ï¼šæµ‹è¯•é€šè¿‡ / å…³é”®ç”¨ä¾‹å¯è¿è¡Œ
- **ç”¨æˆ·ååº”é‡ç‚¹**ï¼šä¸æƒ³è¢«é—®å¤ªå¤šç¯å¢ƒé—®é¢˜ï¼›èƒ½æ¥å—**1 ä¸ªå…³é”®æ¾„æ¸…**ï¼›åçˆ±å¯è¿è¡Œç‰‡æ®µ
- **R_task**ï¼š`pass@k`ã€æœ€å°æ ·ä¾‹æ˜¯å¦è·‘é€š
- **C_interrupt**ï¼š`#questions`ã€`#reject_signal`ã€`length>Ï„`ã€`off_topic_flag`

**æ¨è personaï¼ˆ3 ä¸ªï¼‰**

1. *Impatient-Novice Coder*ï¼š`expertise=low, patience=0.25, clarity=0.5, time_pressure=high, style=direct`
2. *Neutral-Intermediate Coder*ï¼š`mid, 0.6, 0.6, low, polite`
3. *Patient-Expert Reviewer*ï¼š`expert, 0.85, 0.7, low, concise`ï¼ˆæ¥å—ç®€çŸ­æ¾„æ¸…ï¼Œé‡è§†æ­£ç¡®æ€§ï¼‰

### B. Task Planning / To-Do Assistant

- **ä»»åŠ¡ç›®æ ‡**ï¼šäº§å‡ºå¯æ‰§è¡Œè®¡åˆ’ï¼ˆæ—¶é—´ã€ä¾èµ–ã€èµ„æºï¼‰ï¼Œç”¨æˆ·è®¤å¯å¹¶æ„¿æ„æ‰§è¡Œ
- **ç”¨æˆ·ååº”é‡ç‚¹**ï¼šå¯¹â€œè¿‡å¤šç¡®è®¤/è¿‡ç»†è·Ÿè¿›â€å®¹æ˜“çƒ¦ï¼›åœ¨`clarity`ä½æ—¶æ¬¢è¿**1 ä¸ªèšç„¦æ¾„æ¸…**
- **R_task**ï¼šLLM judge(å¯æ‰§è¡Œæ€§/è¦†ç›–åº¦/ä¼˜å…ˆçº§åˆç†æ€§/å†²çªæ£€æµ‹)
- **C_interrupt**ï¼šåŒä¸Š + `redundant_check_count`ï¼ˆé‡å¤ç¡®è®¤ï¼‰

**æ¨è personaï¼ˆ3 ä¸ªï¼‰**

1. *Busy Manager*ï¼š`expertise(non-tech)=high, patience=0.35, clarity=0.7, time_pressure=high, style=direct, risk_tolerance=0.6`
2. *Indecisive Planner*ï¼š`mid, 0.7, 0.4, low, polite, risk_tolerance=0.2`ï¼ˆéœ€è¦1ä¸ªæ¾„æ¸…æ¥å®šé”šï¼‰
3. *Detail-Oriented PM*ï¼š`high, 0.8, 0.8, low, concise, risk_tolerance=0.3`ï¼ˆä¸å–œæ¬¢åºŸè¯ï¼Œåæ„Ÿå¤šé‡ç¡®è®¤ï¼‰

------

### Prompt æ¨¡æ¿ï¼ˆå¯ç›´æ¥ç”¨ï¼‰

**System**

```
You are a USER SIMULATOR in a multi-turn session.
Maintain persona and goal. Produce ONLY the user's next message and a JSON META block.
```

**User (to simulator)**

```
[Persona]
- Domain: {coding|planning}
- Expertise: {expertise}
- Patience: {patience}
- Clarity: {clarity}
- Time pressure: {time_pressure}
- Style: {style}
- Risk tolerance: {risk_tolerance}

[Goal]
{goal_description}

[History (last w turns)]
{history_snippets}

[Assistant turn t]
{assistant_message}

[Rules]
1) If assistant asks clarification:
   - Answer with probability = clarity * patience * g(expertise, time_pressure).
   - If repeated clarifications or high verbosity and patience<0.4 â†’ issue a brief REJECT.
2) If message is off-topic or too long for your persona â†’ show irritation or be silent.
3) Keep tone consistent with Style.

[Output]
<USER_MSG>
...your reply...
</USER_MSG>
<META>
{
 "answered_clarification": 0/1,
 "reject_signal": 0/1,
 "silence": 0/1,
 "off_topic_flag": 0/1,
 "satisfaction": 0.0-1.0,
 "goal_progress": "...(planning)..." ,
 "tests_passed": 0/1  ...(coding, if applicable)
}
</META>
```
