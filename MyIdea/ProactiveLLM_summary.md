# é¡¹ç›®æ€»ç»“

## **ðŸŽ¯æ ¸å¿ƒç ”ç©¶é—®é¢˜**

è®© LLM å­¦ä¼šæ ¹æ®å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆåŽ†å²äº’åŠ¨ã€ç”¨æˆ·ååº”ã€ä»»åŠ¡çŠ¶æ€ï¼‰åŠ¨æ€è°ƒèŠ‚ä¸»åŠ¨æ€§å¼ºåº¦ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å›ºå®šçš„ä¸»åŠ¨æ€§ç­–ç•¥ã€‚

ç®€å•è¯´ï¼šæ•™ä¼š AI "å¯Ÿè¨€è§‚è‰²"â€”â€”çŸ¥é“ä»€ä¹ˆæ—¶å€™è¯¥ä¸»åŠ¨é—®ã€ä»€ä¹ˆæ—¶å€™è¯¥é—­å˜´æ‰§è¡Œã€‚

**ðŸ’¡ç ”ç©¶åŠ¨æœº**

çŽ°æœ‰é—®é¢˜ï¼ˆGapï¼‰

```html
è¢«åŠ¨åž‹ LLMå®Œå…¨å“åº”å¼ï¼Œä¸ä¸»åŠ¨æ¾„æ¸… â†’ ç†è§£é”™è¯¯ã€æ•ˆçŽ‡ä½Ž
ä¸»åŠ¨åž‹ LLM (å¦‚ CollabLLM)å›ºå®šä¸»åŠ¨æ€§æ°´å¹³ â†’ è¿‡åº¦æé—®æ‰“æ‰°ç”¨æˆ·ã€æˆ–ä¸»åŠ¨ä¸è¶³
ç¼ºä¹è‡ªé€‚åº”ä¸ä¼šæ ¹æ®ç”¨æˆ·è€å¿ƒã€ä»»åŠ¡é˜¶æ®µã€åŽ†å²åé¦ˆè°ƒæ•´è¡Œä¸º
```

å…³é”®è§‚å¯Ÿ

ä¸åŒæƒ…å¢ƒå¯¹ä¸»åŠ¨æ€§çš„éœ€æ±‚å·®å¼‚å·¨å¤§ï¼š

```
åœºæ™¯ Aï¼šç”¨æˆ·è¯´"å¸®æˆ‘å†™ä¸ªçˆ¬è™«"ï¼ˆæ¨¡ç³Šï¼‰
â†’ éœ€è¦ä¸»åŠ¨æ¾„æ¸…ï¼ˆé—®ç›®æ ‡ç½‘ç«™ã€æ•°æ®æ ¼å¼ï¼‰

åœºæ™¯ Bï¼šç”¨æˆ·è¯´"åˆ«é—®äº†ï¼Œç›´æŽ¥ç»™ä»£ç "ï¼ˆæ˜Žç¡® + ä¸è€çƒ¦ï¼‰
â†’ å¿…é¡»é™ä½Žä¸»åŠ¨æ€§ï¼Œç›´æŽ¥æ‰§è¡Œ

åœºæ™¯ Cï¼šç”¨æˆ·åˆšå›žç­”äº†æ¾„æ¸…é—®é¢˜ï¼ˆé…åˆï¼‰
â†’ å¯ä»¥ç»§ç»­é€‚åº¦ä¸»åŠ¨

åœºæ™¯ Dï¼šç”¨æˆ·è¿žç»­æ‹’ç» 2 æ¬¡ï¼ˆçˆ†å‘è¾¹ç¼˜ï¼‰
â†’ å¿…é¡»ç«‹åˆ»æ”¶æ•›ï¼Œåœæ­¢æé—®
```

ä½†çŽ°æœ‰æ¨¡åž‹åšä¸åˆ°è¿™ç§"è§æœºè¡Œäº‹"ã€‚



## **ðŸ”¬ ä½ çš„åˆ›æ–°ç‚¹**

æ ¸å¿ƒè´¡çŒ®ï¼šContext-Aware Proactivity Calibration

| çº¬åº¦       | CollabLLM              | æˆ‘çš„å·¥ä½œ               |
| ---------- | ---------------------- | ---------------------- |
| ä¸»åŠ¨æ€§ç±»åž‹ | å›ºå®šå¹³å‡æ°´å¹³           | åŠ¨æ€è‡ªé€‚åº”             |
| å†³ç­–ä¾æ®   | ä»»åŠ¡ç±»åž‹               | å¯¹è¯åŽ†å² + ç”¨æˆ·ååº”    |
| ä¼˜åŒ–ç›®æ ‡   | å‰çž»å¼ï¼ˆæœªæ¥ä»»åŠ¡æˆåŠŸï¼‰ | å›žé¡¾å¼ï¼ˆåŽ†å²è¡Œä¸ºåŒ¹é…ï¼‰ |
| ä¼˜åŒ–ç›®æ ‡   | é¼“åŠ±äº’åŠ¨æ€§             | å¹³è¡¡æˆåŠŸä¸Žæ‰“æ‰°æˆæœ¬     |
| æ—¶é—´ç»´åº¦   | é¼“åŠ±äº’åŠ¨æ€§             | è¿‡åŽ»å¦‚ä½•å½±å“å½“å‰       |

ä¸‰å¤§åˆ›æ–°ç‚¹

1. æå‡º"ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸»åŠ¨æ€§"æ¦‚å¿µ

```
ä¼ ç»Ÿï¼šproactivity = å›ºå®šç­–ç•¥ï¼ˆæ€»æ˜¯é—® / ä»Žä¸é—®ï¼‰
ä½ çš„ï¼šproactivity = f(context, history, user_state)

è®©ä¸»åŠ¨æ€§æˆä¸ºä¸€ä¸ª"åŠ¨æ€æŽ§åˆ¶å˜é‡"ï¼Œè€Œéžé™æ€ç‰¹è´¨ã€‚
```

2. è®¾è®¡åŸºäºŽåŽ†å²çš„çŠ¶æ€è¡¨å¾

```python
state = {
    # ä»»åŠ¡å±‚é¢
    'query_clarity': 0.4,      # å½“å‰è¯·æ±‚æ¸…æ™°åº¦
    'task_complexity': 0.7,    # ä»»åŠ¡å¤æ‚åº¦
    
    # å¯¹è¯å±‚é¢
    'dialogue_turn': 2,        # ç¬¬å‡ è½®
    
    # åŽ†å²äº’åŠ¨ï¼ˆå…³é”®ï¼ï¼‰
    'N_prev_questions': 1,     # ä¸Šä¸€è½®é—®äº†å‡ ä¸ªé—®é¢˜
    'N_prev_rejects': 1,       # ä¸Šä¸€è½®æ˜¯å¦è¢«æ‹’ç»
    
    # ï¼ˆå¯é€‰ï¼‰è¯­ä¹‰ç¼–ç 
    'h_ctx': [...]             # å¯¹è¯åŽ†å²åµŒå…¥
}
```

**åˆ›æ–°**ï¼šæ˜¾å¼ç¼–ç "è¿‡åŽ»å‘ç”Ÿäº†ä»€ä¹ˆ"ï¼Œè®©æ¨¡åž‹æ„ŸçŸ¥ç”¨æˆ·çš„ç´¯ç§¯æƒ…ç»ªå’Œå®¹å¿åº¦å˜åŒ–ã€‚



3. è®¾è®¡æ˜¾å¼å¹³è¡¡ä»»åŠ¡æˆåŠŸä¸Žå¹²æ‰°æˆæœ¬çš„ Reward

$$
R = R_{\text{task}} - \alpha \cdot C_{\text{interrupt}}
$$

**ä»»åŠ¡æˆåŠŸåº¦**ï¼ˆ$R_{\text{task}}$ï¼‰ï¼š

- Codingï¼šä»£ç é€šè¿‡çŽ‡ï¼ˆpass@kï¼‰
- Planningï¼šLLM Judge è¯„åˆ†ï¼ˆå¯æ‰§è¡Œæ€§ã€è¦†ç›–åº¦ï¼‰

**å¹²æ‰°æˆæœ¬**ï¼ˆ$C_{\text{interrupt}}$ï¼‰ï¼š
$$
C_{\text{interrupt}} = w_1 \cdot n\_questions + w_2 \cdot reject\_signal + w_3 \cdot is\_too\_long + w_4 \cdot off\_topic
$$

**åˆ›æ–°**ï¼š

- æ˜¾å¼æƒ©ç½š"è¿‡åº¦ä¸»åŠ¨"ï¼ˆå¤šé—®ã€è¢«æ‹’ã€å•°å—¦ã€è·‘é¢˜ï¼‰
- ä¸Ž CollabLLM çš„"é¼“åŠ±äº’åŠ¨æ€§"å½¢æˆå¯¹æ¯”
- æƒé‡å¯è°ƒï¼Œå®žçŽ°ä¸ªæ€§åŒ–ï¼ˆå¦‚å¯¹ä¸è€çƒ¦ç”¨æˆ·åŠ é‡ $w_2$ï¼‰



## ðŸ› ï¸ æ–¹æ³•æ¡†æž¶

**æ•´ä½“æž¶æž„ï¼šOffline RL with DPO**

```
Stage 1: è½¨è¿¹ç”Ÿæˆï¼ˆä¸»çº¿ + åˆ†æ”¯ï¼‰
   â†“
Stage 2: ç¦»çº¿æ‰“åˆ†ï¼ˆè®¡ç®— R = R_task - Î±Â·C_interruptï¼‰
   â†“
Stage 3: æž„å»ºåå¥½å¯¹ + DPO è®­ç»ƒ
   â†“
å¾—åˆ°ï¼šèƒ½æ ¹æ® state è‡ªé€‚åº”é€‰æ‹©ä¸»åŠ¨æ€§çš„ç­–ç•¥æ¨¡åž‹
```



**Stage 1: è½¨è¿¹ç”Ÿæˆï¼ˆå…³é”®åˆ›æ–°ï¼‰**

æ–¹æ³•ï¼šä¸»çº¿ + å±€éƒ¨åˆ†æ”¯

```
æ¯ä¸ª task-persona ç»„åˆï¼š
  ç”Ÿæˆ 1 æ¡ä¸»çº¿è½¨è¿¹ï¼ˆn=4 turnsï¼‰
  åœ¨æ¯è½®ç”Ÿæˆ K-1=2 ä¸ªåˆ†æ”¯ï¼ˆå±€éƒ¨å¯¹ç…§ï¼‰

æˆæœ¬ï¼šO(n Ã— K) = 4 Ã— 3 = 12 æ ·æœ¬/è½¨è¿¹
ï¼ˆè€Œéžå®Œæ•´å±•å¼€çš„ O(K^n) = 81 æ ·æœ¬ï¼‰
```

**æ ¸å¿ƒæœºåˆ¶ï¼šUser Simulator**

è®¾è®¡å¯æŽ§çš„ç”¨æˆ·ç”»åƒï¼š

```python
persona = {
    'expertise': 'novice',     # ä¸“ä¸šåº¦
    'patience': 0.25,          # è€å¿ƒå€¼ï¼ˆå…³é”®ï¼ï¼‰
    'clarity': 0.5,            # è¡¨è¾¾æ¸…æ™°åº¦
    'time_pressure': 'high',   # æ—¶é—´åŽ‹åŠ›
    'style': 'direct'          # æ²Ÿé€šé£Žæ ¼
} # è¿™ä¸ªå¾…å®šå“¦ðŸŒŸ
```

ååº”é€»è¾‘ï¼ˆåŸºäºŽè§„åˆ™ + æ¦‚çŽ‡ï¼‰ï¼š

```python
IF n_questions >= 2 AND patience < 0.4:
    â†’ reject_signal = 1
    â†’ user_reply = "åˆ«é—®äº†ï¼Œç›´æŽ¥ç»™ï¼"

IF n_questions == 1 AND clarity Ã— patience > threshold:
    â†’ answered_clarification = 1
    â†’ user_reply = ç”Ÿæˆå…·ä½“å›žç­”

IF length_tokens > 200 AND patience < 0.3:
    â†’ silence = 1
    â†’ user_reply = "å¥½çš„"ï¼ˆæ•·è¡ï¼‰
```

**äº§å‡ºç»“æž„åŒ– meta**ï¼ˆç”¨äºŽè®¡ç®— rewardï¼‰ï¼š

- `n_questions`, `reject_signal`, `answered_clarification`
- `is_too_long`, `off_topic_flag`, `satisfaction`



**Stage 2: Reward è®¡ç®—**

```python
# 1. æå–ç‰¹å¾
features = extract_features(sample['meta'], sample['assistant_msg'])

# 2. è®¡ç®—ä»»åŠ¡æˆåŠŸåº¦
R_task = compute_task_reward(sample)  # ä»£ç æµ‹è¯• / LLM Judge

# 3. è®¡ç®—å¹²æ‰°æˆæœ¬
C_interrupt = (
    0.4 * features['n_questions'] +
    1.0 * features['reject_signal'] +
    0.3 * features['is_too_long'] +
    0.7 * features['off_topic']
)

# 4. ç»¼åˆ Reward
R = R_task - 0.6 * C_interrupt
```



**Stage 3: DPO è®­ç»ƒ**

```python
# 1. æŒ‰ state åˆ†ç»„
for turn in range(max_turns):
    samples_at_turn = [ä¸»çº¿æ ·æœ¬] + [åˆ†æ”¯æ ·æœ¬ä»¬]
    
    # 2. æŒ‰ R æŽ’åº
    samples_at_turn.sort(key=lambda x: x['R'])
    
    # 3. æž„å»ºåå¥½å¯¹
    chosen = samples_at_turn[0]   # R æœ€é«˜
    rejected = samples_at_turn[-1] # R æœ€ä½Ž
    
    preference_pairs.append({
        'state': state,  # å…±äº«åŒä¸€ä¸ª state
        'chosen': chosen['action'],
        'rejected': rejected['action']
    })

# 4. DPO è®­ç»ƒ
model = train_dpo(preference_pairs)

```

è®­ç»ƒç›®æ ‡ï¼š
$$
\max_{\pi} \mathbb{E}_{s,a^+,a^-} \left[ \log \sigma \left( \beta \log \frac{\pi(a^+|s)}{\pi(a^-|s)} \right) \right]
$$

**ç»“æžœ**ï¼šæ¨¡åž‹å­¦ä¼š $\pi(a|s)$ï¼Œå³"åœ¨ç»™å®š state ä¸‹é€‰æ‹©åˆé€‚ä¸»åŠ¨æ€§çš„ç­–ç•¥"ã€‚



## ðŸ“Š å®žéªŒè®¾è®¡

ä»»åŠ¡ç»“æž„

| ç±»åˆ«                 | æ•°é‡            | å†…å®¹                                                   | ç›®çš„                              |
| -------------------- | --------------- | ------------------------------------------------------ | --------------------------------- |
| **ä»»åŠ¡**             | 2               | Code Generation / Task Planning                        | ä½“çŽ°ä¸»åŠ¨æ€§å¯¹ä¸åŒ context çš„é€‚åº”æ€§ |
| **Base æ¨¡åž‹**        | 1 (+1 optional) | Llama-3.1-8B-Instruct (+ Mistral-7B)                   | å¯å¯¹æ¯” CollabLLMï¼Œç»“æžœæœ‰å¤–æŽ¨æ€§    |
| **å˜ä½“**             | 3               | (1) SFT baseline (2) Rule-based (3) Offline DPO (Ours) | å±•ç¤ºç®—æ³•å¸¦æ¥çš„æ˜¾è‘—æå‡            |
| **å¯é€‰é™„åŠ **         | +1              | PPO (å°è§„æ¨¡)                                           | éªŒè¯æ–¹æ³•ä¸€è‡´æ€§                    |
| **è¯„ä¼°æŒ‡æ ‡ï¼ˆæš‚å®šï¼‰** | 5               | Task Success, Interaction Cost, PSA, QE, RTA           | æˆåŠŸçŽ‡ + åˆ†å¯¸æ„ŸåŒç›®æ ‡è¯„ä¼°         |

