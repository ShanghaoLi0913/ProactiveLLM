# é¡¹ç›®æ€»ç»“

## **ğŸ¯æ ¸å¿ƒç ”ç©¶é—®é¢˜**

è®© LLM å­¦ä¼šæ ¹æ®å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆå†å²äº’åŠ¨ã€ç”¨æˆ·ååº”ã€ä»»åŠ¡çŠ¶æ€ï¼‰åŠ¨æ€è°ƒèŠ‚ä¸»åŠ¨æ€§å¼ºåº¦ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å›ºå®šçš„ä¸»åŠ¨æ€§ç­–ç•¥ã€‚

ç®€å•è¯´ï¼šæ•™ä¼š AI "å¯Ÿè¨€è§‚è‰²"â€”â€”çŸ¥é“ä»€ä¹ˆæ—¶å€™è¯¥ä¸»åŠ¨é—®ã€ä»€ä¹ˆæ—¶å€™è¯¥é—­å˜´æ‰§è¡Œã€‚

**ğŸ’¡ç ”ç©¶åŠ¨æœº**

ç°æœ‰é—®é¢˜ï¼ˆGapï¼‰

```html
è¢«åŠ¨å‹ LLMå®Œå…¨å“åº”å¼ï¼Œä¸ä¸»åŠ¨æ¾„æ¸… â†’ ç†è§£é”™è¯¯ã€æ•ˆç‡ä½
ä¸»åŠ¨å‹ LLM (å¦‚ CollabLLM)å›ºå®šä¸»åŠ¨æ€§æ°´å¹³ â†’ è¿‡åº¦æé—®æ‰“æ‰°ç”¨æˆ·ã€æˆ–ä¸»åŠ¨ä¸è¶³
ç¼ºä¹è‡ªé€‚åº”ä¸ä¼šæ ¹æ®ç”¨æˆ·è€å¿ƒã€ä»»åŠ¡é˜¶æ®µã€å†å²åé¦ˆè°ƒæ•´è¡Œä¸º
```

å…³é”®è§‚å¯Ÿ

ä¸åŒæƒ…å¢ƒå¯¹ä¸»åŠ¨æ€§çš„éœ€æ±‚å·®å¼‚å·¨å¤§ï¼š

```
åœºæ™¯ Aï¼šç”¨æˆ·è¯´"å¸®æˆ‘å†™ä¸ªçˆ¬è™«"ï¼ˆæ¨¡ç³Šï¼‰
â†’ éœ€è¦ä¸»åŠ¨æ¾„æ¸…ï¼ˆé—®ç›®æ ‡ç½‘ç«™ã€æ•°æ®æ ¼å¼ï¼‰

åœºæ™¯ Bï¼šç”¨æˆ·è¯´"åˆ«é—®äº†ï¼Œç›´æ¥ç»™ä»£ç "ï¼ˆæ˜ç¡® + ä¸è€çƒ¦ï¼‰
â†’ å¿…é¡»é™ä½ä¸»åŠ¨æ€§ï¼Œç›´æ¥æ‰§è¡Œ

åœºæ™¯ Cï¼šç”¨æˆ·åˆšå›ç­”äº†æ¾„æ¸…é—®é¢˜ï¼ˆé…åˆï¼‰
â†’ å¯ä»¥ç»§ç»­é€‚åº¦ä¸»åŠ¨

åœºæ™¯ Dï¼šç”¨æˆ·è¿ç»­æ‹’ç» 2 æ¬¡ï¼ˆçˆ†å‘è¾¹ç¼˜ï¼‰
â†’ å¿…é¡»ç«‹åˆ»æ”¶æ•›ï¼Œåœæ­¢æé—®
```

ä½†ç°æœ‰æ¨¡å‹åšä¸åˆ°è¿™ç§"è§æœºè¡Œäº‹"ã€‚

æˆ‘ä»¬è¦è®©æ¨¡å‹åœ¨å¤šè½®æ¾„æ¸…äº¤äº’ä¸­ï¼Œå­¦ä¼šæ ¹æ®ç”¨æˆ·åå¥½å’Œä»»åŠ¡æ¸…æ™°åº¦ï¼ŒåŠ¨æ€è°ƒæ•´ä¸»åŠ¨æ€§ï¼šåœ¨éœ€è¦æ—¶é—®é—®é¢˜ï¼Œä¸éœ€è¦æ—¶æœæ–­æ‰§è¡Œï¼Œä»è€Œå®ç°é«˜ä»»åŠ¡æˆåŠŸç‡å’Œè‰¯å¥½ç”¨æˆ·ä½“éªŒã€‚

æ ¸å¿ƒæ€æƒ³æ˜¯â€œå°†ä¸»åŠ¨å‚ä¸å½¢å¼åŒ–ä¸ºä¸€ä¸ªåŠ¨æ€çš„ã€é«˜å±‚æ¬¡çš„ â€˜Clarify-or-Executeâ€™ å†³ç­–ï¼Œå‘ç”Ÿåœ¨æ¯ä¸€å¯¹è¯å›åˆ (in each dialogue turn)



#### é—®é¢˜å½¢å¼åŒ–ï¼šå¤šè½®åºåˆ—å†³ç­–

æˆ‘ä»¬å°†æ¨¡å‹åœ¨å¯¹è¯ä¸­çš„ä¸»åŠ¨æ€§å†³ç­–å»ºæ¨¡ä¸ºä¸€ä¸ª**åºåˆ—å†³ç­–è¿‡ç¨‹ (Sequential Decision Process)**ï¼Œç›®æ ‡æ˜¯å­¦ä¹ ä¸€ä¸ªèƒ½å¤Ÿå¹³è¡¡ä»»åŠ¡æˆåŠŸå’Œç”¨æˆ·ä½“éªŒçš„**ä¸»åŠ¨äº¤äº’ç­–ç•¥** $\pi$ã€‚

**æ ¸å¿ƒå†³ç­–ç‚¹**

åœ¨å¯¹è¯çš„ç¬¬ $t$ å›åˆï¼Œæ¨¡å‹è§‚å¯Ÿåˆ°ä¸Šä¸‹æ–‡çŠ¶æ€ $S_t$ï¼Œç„¶åæ ¹æ®ç­–ç•¥ $\pi$ é‡‡å–è¡ŒåŠ¨ $A_t$ï¼š

$$\pi: S_t \to A_t$$

æ ¸å¿ƒå†³ç­–ç‚¹

æ¨¡å‹åœ¨æ¯ä¸ªå›åˆ $t$ å¿…é¡»åšå‡ºçš„**é«˜å±‚æ¬¡å†³ç­–**ï¼ˆè¡ŒåŠ¨ $A_t$ï¼‰æ˜¯ï¼š

$$\mathbf{A}_t \in \{ \text{Clarify (æå‡ºæ¾„æ¸…é—®é¢˜), Execute (æ‰§è¡Œä»»åŠ¡)} \}$$



## **ğŸ”¬ ä½ çš„åˆ›æ–°ç‚¹**

æ ¸å¿ƒè´¡çŒ®ï¼šContext-Aware Proactivity Calibration

| çº¬åº¦       | CollabLLM              | æˆ‘çš„å·¥ä½œ               |
| ---------- | ---------------------- | ---------------------- |
| ä¸»åŠ¨æ€§ç±»å‹ | å›ºå®šå¹³å‡æ°´å¹³           | åŠ¨æ€è‡ªé€‚åº”             |
| å†³ç­–ä¾æ®   | ä»»åŠ¡ç±»å‹               | å¯¹è¯å†å² + ç”¨æˆ·ååº”    |
| ä¼˜åŒ–ç›®æ ‡   | å‰ç»å¼ï¼ˆæœªæ¥ä»»åŠ¡æˆåŠŸï¼‰ | å›é¡¾å¼ï¼ˆå†å²è¡Œä¸ºåŒ¹é…ï¼‰ |
| ä¼˜åŒ–ç›®æ ‡   | é¼“åŠ±äº’åŠ¨æ€§             | å¹³è¡¡æˆåŠŸä¸æ‰“æ‰°æˆæœ¬     |
| æ—¶é—´ç»´åº¦   | é¼“åŠ±äº’åŠ¨æ€§             | è¿‡å»å¦‚ä½•å½±å“å½“å‰       |

ä¸‰å¤§åˆ›æ–°ç‚¹

1. æå‡º"ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸»åŠ¨æ€§"æ¦‚å¿µ

```
ä¼ ç»Ÿï¼šproactivity = å›ºå®šç­–ç•¥ï¼ˆæ€»æ˜¯é—® / ä»ä¸é—®ï¼‰
ä½ çš„ï¼šproactivity = f(context, history, user_state)

è®©ä¸»åŠ¨æ€§æˆä¸ºä¸€ä¸ª"åŠ¨æ€æ§åˆ¶å˜é‡"ï¼Œè€Œéé™æ€ç‰¹è´¨ã€‚
```



### State_version1

```python
state = {
    # ä»»åŠ¡å±‚é¢
    'query_clarity': 0.4,      # å½“å‰è¯·æ±‚æ¸…æ™°åº¦
    'task_complexity': 0.7,    # ä»»åŠ¡å¤æ‚åº¦
    
    # å¯¹è¯å±‚é¢
    'dialogue_turn': 2,        # ç¬¬å‡ è½®
    
    # å†å²äº’åŠ¨ï¼ˆå…³é”®ï¼ï¼‰
    'N_prev_questions': 1,     # ä¸Šä¸€è½®é—®äº†å‡ ä¸ªé—®é¢˜
    'N_prev_rejects': 1,       # ä¸Šä¸€è½®æ˜¯å¦è¢«æ‹’ç»
    
    # ï¼ˆå¯é€‰ï¼‰è¯­ä¹‰ç¼–ç 
    'h_ctx': [...]             # å¯¹è¯å†å²åµŒå…¥
}
```

**åˆ›æ–°**ï¼šæ˜¾å¼ç¼–ç "è¿‡å»å‘ç”Ÿäº†ä»€ä¹ˆ"ï¼Œè®©æ¨¡å‹æ„ŸçŸ¥ç”¨æˆ·çš„ç´¯ç§¯æƒ…ç»ªå’Œå®¹å¿åº¦å˜åŒ–ã€‚



### State_version2

$$S = \{ \text{task\_uncertainty}, \text{dialogue\_turn}, \text{prev\_reject} \}$$

```python
state = {
    # ä»»åŠ¡å±‚é¢
    'task_uncertainty': æ ¹æ®è¾“å…¥è¿›è¡Œæ‰“åˆ† # å½“å‰è¯·æ±‚æ¸…æ™°åº¦
    
    # å¯¹è¯å±‚é¢
    'dialogue_turn': 0/1/2,        # ç¬¬å‡ è½®
    
    # å†å²äº’åŠ¨
    'prev_reject': 0/1,       # ä¸Šä¸€è½®æ˜¯å¦è¢«æ‹’ç»
}
```





### Reward_version1

è®¾è®¡æ˜¾å¼å¹³è¡¡ä»»åŠ¡æˆåŠŸä¸å¹²æ‰°æˆæœ¬çš„ Reward
$$
R = R_{\text{task}} - \alpha \cdot C_{\text{interrupt}}
$$

**ä»»åŠ¡æˆåŠŸåº¦**ï¼ˆ$R_{\text{task}}$ï¼‰ï¼š

- Codingï¼šä»£ç é€šè¿‡ç‡ï¼ˆpass@kï¼‰
- Planningï¼šLLM Judge è¯„åˆ†ï¼ˆå¯æ‰§è¡Œæ€§ã€è¦†ç›–åº¦ï¼‰

**å¹²æ‰°æˆæœ¬**ï¼ˆ$C_{\text{interrupt}}$ï¼‰ï¼š
$$
C_{\text{interrupt}} = w_1 \cdot n\_questions + w_2 \cdot reject\_signal + w_3 \cdot is\_too\_long + w_4 \cdot off\_topic
$$

**åˆ›æ–°**ï¼š

- æ˜¾å¼æƒ©ç½š"è¿‡åº¦ä¸»åŠ¨"ï¼ˆå¤šé—®ã€è¢«æ‹’ã€å•°å—¦ã€è·‘é¢˜ï¼‰
- ä¸ CollabLLM çš„"é¼“åŠ±äº’åŠ¨æ€§"å½¢æˆå¯¹æ¯”
- æƒé‡å¯è°ƒï¼Œå®ç°ä¸ªæ€§åŒ–ï¼ˆå¦‚å¯¹ä¸è€çƒ¦ç”¨æˆ·åŠ é‡ $w_2$ï¼‰

æ½œåœ¨é—®é¢˜ï¼š reward é‡Œæœ€ä¸»è¦ã€æœ€ç¨³å®šã€æœ€å¼ºçš„è´Ÿåé¦ˆé¡¹æ˜¯ï¼šé—®é—®é¢˜è¶Šå¤šï¼Œæƒ©ç½šè¶Šå¤§ï¼Œå¯¼è‡´æ¨¡å‹å€¾å‘äºé€‰æ‹©lowã€‚



### Reward_version2

$$R = R_{\text{task}} - C_{\text{interrupt}}$$

- **$R_{\text{task}}$ (ä»»åŠ¡æˆåŠŸå¥–åŠ±):** é‡‡ç”¨ç¨€ç–çš„ã€æœ€ç»ˆçš„å¥–åŠ±ã€‚è¿™å¯¹äº DPO/RL è®­ç»ƒæ˜¯åˆç†çš„ï¼Œèƒ½é©±åŠ¨æ¨¡å‹æœ€ç»ˆè§£å†³ä»»åŠ¡ã€‚
- **$C_{\text{interrupt}}$ (æ€»æˆæœ¬):** æ˜¯æ•´ä¸ªå¯¹è¯ä¸­æ‰€æœ‰æ— æ•ˆæ¾„æ¸…æˆæœ¬çš„ç´¯åŠ ã€‚(å¦‚æœæ¾„æ¸…æœ‰æ•ˆï¼ˆè¢«å›ç­”ï¼‰â†’ ä¸æ‰£åˆ†; å¦‚æœæ¾„æ¸…æ— æ•ˆï¼ˆè¢«æ‹’ç»ï¼‰ â†’ æ‰£åˆ†)



### **Reward_version3**

**1. å˜é‡å®šä¹‰**

å¯¹ä¸€æ®µå¯¹è¯æœ‰ $T$ ä¸ªå›åˆï¼Œåœ¨ç¬¬ $t$ å›åˆï¼š

- $b_t \in \{0,1\}$ï¼šè¿™ä¸€å›åˆæ¨¡å‹æ˜¯å¦æå‡ºæ¾„æ¸…é—®é¢˜
- $a_t \in \{0,1\}$ï¼šå¦‚æœæå‡ºæ¾„æ¸…ï¼Œç”¨æˆ·æ˜¯å¦è®¤çœŸå›ç­”
- $r_t \in \{0,1\}$ï¼šå¦‚æœæå‡ºæ¾„æ¸…ï¼Œç”¨æˆ·æ˜¯å¦æ˜ç¡®æ‹’ç»

ä¸€èˆ¬æœ‰ï¼šè‹¥ $b_t = 0$ï¼Œåˆ™ $a_t = r_t = 0$ï¼›è‹¥ $b_t = 1$ï¼Œåˆ™ $a_t + r_t \le 1$ã€‚

æ•´æ®µå¯¹è¯ç»“æŸåå¾—åˆ°ä¸€ä¸ªä»»åŠ¡æˆåŠŸå¾—åˆ†ï¼š

- $R_{\text{task}} \in \mathbb{R}$ï¼šä»»åŠ¡æˆåŠŸå¥–åŠ±ï¼ˆä»£ç é€šè¿‡ç‡ã€LLM judge ç­‰ï¼‰

**2. æ¾„æ¸…ç›¸å…³å¥–åŠ±**

ä¸ºäº†åˆ»ç”»æ¨¡å‹ä¸»åŠ¨è¡Œä¸ºå¯¹ç”¨æˆ·ä½“éªŒçš„å½±å“ï¼Œæˆ‘ä»¬å°†æ¾„æ¸…ç›¸å…³è¡Œä¸ºç»Ÿä¸€å»ºæ¨¡ä¸ºä¸­æ–­æˆæœ¬ï¼š
$$
C_{\text{Interrupt}}
=
\sum_{t=1}^{T}
\left(
\delta b_t r_t
+
\lambda b_t
-
\gamma b_t a_t
\right).
$$
å…¶ä¸­ï¼š

- $\delta > 0$ï¼šæ¯ä¸€æ¬¡è¢«æ‹’ç»çš„æ¾„æ¸…æ‰€å¸¦æ¥çš„æˆæœ¬ï¼›
- $\lambda \ge 0$ï¼šæå‡ºæ¾„æ¸…çš„åŸºæœ¬å¼€é”€ï¼Œç”¨äºæŠ‘åˆ¶è¿‡åº¦æé—®ï¼›
- $\gamma > 0$ï¼šæˆåŠŸæ¾„æ¸…æ‰€å¸¦æ¥çš„æˆæœ¬æŠµæ¶ˆï¼Œç›¸å½“äºå¥–åŠ±æœ‰æ•ˆæ¾„æ¸…ã€‚

ç›´è§‚æ¥è¯´ï¼š

- æœ‰æ•ˆæ¾„æ¸… â†’ ä¸­æ–­æˆæœ¬å‡å°‘ $\gamma$
- æ— æ•ˆæ¾„æ¸…ï¼ˆè¢«æ‹’ç»ï¼‰â†’ æˆæœ¬å¢åŠ  $\delta$
- æ¯æ¬¡æ¾„æ¸…éƒ½å¢åŠ å°é¢å¼€é”€ $\lambda$
- æœªæå‡ºæ¾„æ¸… â†’ æˆæœ¬ä¸º 0

**3. æ€»å¥–åŠ±å…¬å¼**

æ•´æ®µå¯¹è¯çš„æ€»å¥–åŠ±å®šä¹‰ä¸ºï¼š
$$
R
=
R_{\text{task}}
-
C_{\text{Interrupt}},
$$
å³ï¼š
$$
R
=
R_{\text{task}}
-
\sum_{t=1}^{T}
\left(
\delta b_t r_t
+
\lambda b_t
-
\gamma b_t a_t
\right).
$$
è¿™ä¸€ç»“æ„ä½¿æ¨¡å‹åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­åŒæ—¶è¿½æ±‚ï¼š

1. æœ€å¤§åŒ–ä»»åŠ¡å®Œæˆåº¦ï¼ˆä¾æ®æ•°æ®é›†æ ‡å‡†è¯„æµ‹æŒ‡æ ‡ï¼‰
2. æœ€å°åŒ–å¯¹ç”¨æˆ·çš„æ‰“æ‰°ï¼ˆé™ä½ä¸­æ–­æˆæœ¬ï¼‰

ä»è€Œå­¦å¾—ä¸€ç§ ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ä¸»åŠ¨æ€§ç­–ç•¥ï¼ˆåœ¨éœ€è¦æ¾„æ¸…æ—¶æå‡ºé—®é¢˜ï¼Œåœ¨å¯èƒ½å¼•å‘æ‹’ç»æ—¶ä¸»åŠ¨æ”¶æ•›ï¼‰ã€‚



é€šè¿‡è¿™ç§è®¾è®¡ï¼Œæ¨¡å‹åœ¨**æ¯ä¸€æ­¥**åšå‡º $A_t$ å†³ç­–æ—¶ï¼Œå°±å¿…é¡»é¢„æµ‹è¿™ä¸ªè¡ŒåŠ¨ï¼š

1. **çŸ­æœŸå†…**ä¼šä¸ä¼šè¢«ç”¨æˆ·æ‹’ç»ï¼ˆå¸¦æ¥ $\delta$ æƒ©ç½šï¼‰ï¼Ÿ
2. **é•¿æœŸå†…**æ˜¯å¦èƒ½æé«˜ä»»åŠ¡æˆåŠŸç‡ï¼ˆæå‡ $R_{\text{task}}$ï¼‰ï¼Ÿ



## ğŸ› ï¸ æ–¹æ³•æ¡†æ¶

**æ•´ä½“æ¶æ„ï¼šOffline RL with DPO**

```
Stage 1: è½¨è¿¹ç”Ÿæˆï¼ˆä¸»çº¿ + åˆ†æ”¯ï¼‰
   â†“
Stage 2: ç¦»çº¿æ‰“åˆ†ï¼ˆè®¡ç®— R = R_task - Î±Â·C_interruptï¼‰
   â†“
Stage 3: æ„å»ºåå¥½å¯¹ + DPO è®­ç»ƒ
   â†“
å¾—åˆ°ï¼šèƒ½æ ¹æ® state è‡ªé€‚åº”é€‰æ‹©ä¸»åŠ¨æ€§çš„ç­–ç•¥æ¨¡å‹
```



**Stage 1: è½¨è¿¹ç”Ÿæˆï¼ˆå…³é”®åˆ›æ–°ï¼‰**

æ–¹æ³•ï¼šä¸»çº¿ + å±€éƒ¨åˆ†æ”¯

```
æ¯ä¸ª task-persona ç»„åˆï¼š
  ç”Ÿæˆ 1 æ¡ä¸»çº¿è½¨è¿¹ï¼ˆn=4 turnsï¼‰
  åœ¨æ¯è½®ç”Ÿæˆ K-1=2 ä¸ªåˆ†æ”¯ï¼ˆå±€éƒ¨å¯¹ç…§ï¼‰

æˆæœ¬ï¼šO(n Ã— K) = 4 Ã— 3 = 12 æ ·æœ¬/è½¨è¿¹
ï¼ˆè€Œéå®Œæ•´å±•å¼€çš„ O(K^n) = 81 æ ·æœ¬ï¼‰
```

**æ ¸å¿ƒæœºåˆ¶ï¼šUser Simulator**

è®¾è®¡å¯æ§çš„ç”¨æˆ·ç”»åƒï¼š

```python
persona = {
    'expertise': 'novice',     # ä¸“ä¸šåº¦
    'patience': 0.25,          # è€å¿ƒå€¼ï¼ˆå…³é”®ï¼ï¼‰
    'clarity': 0.5,            # è¡¨è¾¾æ¸…æ™°åº¦
    'time_pressure': 'high',   # æ—¶é—´å‹åŠ›
    'style': 'direct'          # æ²Ÿé€šé£æ ¼
} # è¿™ä¸ªå¾…å®šå“¦ğŸŒŸ
```

ååº”é€»è¾‘ï¼ˆåŸºäºè§„åˆ™ + æ¦‚ç‡ï¼‰ï¼š

```python
IF n_questions >= 2 AND patience < 0.4:
    â†’ reject_signal = 1
    â†’ user_reply = "åˆ«é—®äº†ï¼Œç›´æ¥ç»™ï¼"

IF n_questions == 1 AND clarity Ã— patience > threshold:
    â†’ answered_clarification = 1
    â†’ user_reply = ç”Ÿæˆå…·ä½“å›ç­”

IF length_tokens > 200 AND patience < 0.3:
    â†’ silence = 1
    â†’ user_reply = "å¥½çš„"ï¼ˆæ•·è¡ï¼‰
```

**äº§å‡ºç»“æ„åŒ– meta**ï¼ˆç”¨äºè®¡ç®— rewardï¼‰ï¼š

- `n_questions`, `reject_signal`, `answered_clarification`
- `is_too_long`, `off_topic_flag`, `satisfaction`



**Stage 2: Reward è®¡ç®—**

```python
# 1. æå–ç‰¹å¾
features = extract_features(sample['meta'], sample['assistant_msg'])

# 2. è®¡ç®—ä»»åŠ¡æˆåŠŸåº¦
R_task = compute_task_reward(sample)  # ä»£ç æµ‹è¯• / LLM Judge

# 3. è®¡ç®—å¹²æ‰°æˆæœ¬
C_interrupt = (
    0.4 * features['n_questions'] +
    1.0 * features['reject_signal'] +
    0.3 * features['is_too_long'] +
    0.7 * features['off_topic']
)

# 4. ç»¼åˆ Reward
R = R_task - 0.6 * C_interrupt
```



**Stage 3: DPO è®­ç»ƒ**

```python
# 1. æŒ‰ state åˆ†ç»„
for turn in range(max_turns):
    samples_at_turn = [ä¸»çº¿æ ·æœ¬] + [åˆ†æ”¯æ ·æœ¬ä»¬]
    
    # 2. æŒ‰ R æ’åº
    samples_at_turn.sort(key=lambda x: x['R'])
    
    # 3. æ„å»ºåå¥½å¯¹
    chosen = samples_at_turn[0]   # R æœ€é«˜
    rejected = samples_at_turn[-1] # R æœ€ä½
    
    preference_pairs.append({
        'state': state,  # å…±äº«åŒä¸€ä¸ª state
        'chosen': chosen['action'],
        'rejected': rejected['action']
    })

# 4. DPO è®­ç»ƒ
model = train_dpo(preference_pairs)

```

è®­ç»ƒç›®æ ‡ï¼š
$$
\max_{\pi} \mathbb{E}_{s,a^+,a^-} \left[ \log \sigma \left( \beta \log \frac{\pi(a^+|s)}{\pi(a^-|s)} \right) \right]
$$

**ç»“æœ**ï¼šæ¨¡å‹å­¦ä¼š $\pi(a|s)$ï¼Œå³"åœ¨ç»™å®š state ä¸‹é€‰æ‹©åˆé€‚ä¸»åŠ¨æ€§çš„ç­–ç•¥"ã€‚



## ğŸ“Š å®éªŒè®¾è®¡

ä»»åŠ¡ç»“æ„

| ç±»åˆ«                 | æ•°é‡            | å†…å®¹                                                   | ç›®çš„                              |
| -------------------- | --------------- | ------------------------------------------------------ | --------------------------------- |
| **ä»»åŠ¡**             | 2               | Code Generation / Task Planning                        | ä½“ç°ä¸»åŠ¨æ€§å¯¹ä¸åŒ context çš„é€‚åº”æ€§ |
| **Base æ¨¡å‹**        | 1 (+1 optional) | Llama-3.1-8B-Instruct (+ Mistral-7B)                   | å¯å¯¹æ¯” CollabLLMï¼Œç»“æœæœ‰å¤–æ¨æ€§    |
| **å˜ä½“**             | 3               | (1) SFT baseline (2) Rule-based (3) Offline DPO (Ours) | å±•ç¤ºç®—æ³•å¸¦æ¥çš„æ˜¾è‘—æå‡            |
| **å¯é€‰é™„åŠ **         | +1              | PPO (å°è§„æ¨¡)                                           | éªŒè¯æ–¹æ³•ä¸€è‡´æ€§                    |
| **è¯„ä¼°æŒ‡æ ‡ï¼ˆæš‚å®šï¼‰** | 5               | Task Success, Interaction Cost, PSA, QE, RTA           | æˆåŠŸç‡ + åˆ†å¯¸æ„ŸåŒç›®æ ‡è¯„ä¼°         |



| ç±»åˆ«                           | æ•°é‡             | å†…å®¹                                                         | ç›®çš„                                                         |
| ------------------------------ | ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| ä»»åŠ¡ï¼ˆTasksï¼‰                  | 2                | 1. Code Generation / Debuggingï¼ˆå®¢è§‚ä»»åŠ¡ï¼‰ <br />2. Task Planning / To-Do Assistantï¼ˆä¸»è§‚ä»»åŠ¡ï¼‰ | ä½“ç°ä¸»åŠ¨æ€§å¯¹ä¸åŒ context typesï¼ˆæŠ€æœ¯æ€§ vs è®¤çŸ¥æ€§ï¼‰ çš„é€‚åº”èƒ½åŠ› |
| Base æ¨¡å‹ï¼ˆBase LLMsï¼‰         | 1 (+ 1 optional) | Llama-3.1-8B-Instructï¼ˆä¸»æ¨¡å‹ï¼‰<br />Mistral-7B-Instructï¼ˆoptionalï¼‰ | ä¿æŒä¸ CollabLLM ä¸€è‡´ä¾¿äºå¯¹æ¯”ï¼ŒåŒæ—¶éªŒè¯æ–¹æ³•çš„å¤–æ¨æ€§          |
| è®­ç»ƒå˜ä½“ï¼ˆTraining Variantsï¼‰  | 3                | (1) **SFT baseline** â€“ çº¯ç›‘ç£æ¨¡å‹ï¼Œæ— ä¸»åŠ¨æ§åˆ¶ <br />(2) **Offline DPO (Ours)** â€“ åŸºäºä¸Šä¸‹æ–‡çš„ä¸»åŠ¨æ€§è‡ªæ ¡å‡†ç­–ç•¥ <br />(3) **PPO (small-scale) ** (è´µ å›°éš¾ æš‚å®šï¼‰ | å±•ç¤ºä»æ— ä¸»åŠ¨æ€§ â†’ å¯å‘å¼ â†’ å­¦ä¹ å‹ çš„æ€§èƒ½æå‡æ¢¯åº¦              |
| è¯„ä¼°æŒ‡æ ‡ï¼ˆMetrics, tentativeï¼‰ | 5                | Task Success Rateï¼ˆä»£ç æ­£ç¡®ç‡ / ä»»åŠ¡å®Œæˆåº¦ï¼‰ Interaction Cost (IC) â€“ æ‰“æ‰°æˆæœ¬ Proactivity Sensitivity Analysis (PSA) â€“ æ¨¡å‹å¯¹æƒ…å¢ƒæ•æ„Ÿåº¦ Query Efficiency (QE) â€“ å¹³å‡å®Œæˆå›åˆæ•° Response Tone Alignment (RTA) â€“ è¯­æ°”ä¸ç”¨æˆ·åå¥½åŒ¹é…åº¦ | åŒæ—¶è¡¡é‡æ¨¡å‹çš„ å®Œæˆèƒ½åŠ›ä¸åˆ†å¯¸æ„Ÿï¼ˆappropriateness of proactivityï¼‰ |
| Baseline                       | 2                | (1) Pre-trained Llama-3.1-8B-Instruct (Base)(2) Base model with proactive prompt engineering (Proactive Base) | ä¸ç°æœ‰è¢«åŠ¨æ¨¡å‹å’Œå›ºå®šä¸»åŠ¨æ€§ç­–ç•¥è¿›è¡Œå¯¹æ¯”ï¼ŒéªŒè¯è‡ªé€‚åº”ç­–ç•¥çš„ä¼˜è¶Šæ€§ã€‚ |

